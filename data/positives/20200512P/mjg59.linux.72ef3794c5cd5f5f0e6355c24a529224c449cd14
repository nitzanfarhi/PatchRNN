commit 72ef3794c5cd5f5f0e6355c24a529224c449cd14
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 5 19:28:14 2012 +0900

    kprobes: Inverse taking of module_mutex with kprobe_mutex
    
    Currently module_mutex is taken before kprobe_mutex, but this
    can cause issues when we have kprobes register ftrace, as the ftrace
    mutex is taken before enabling a tracepoint, which currently takes
    the module mutex.
    
    If module_mutex is taken before kprobe_mutex, then we can not
    have kprobes use the ftrace infrastructure.
    
    There seems to be no reason that the kprobe_mutex can't be taken
    before the module_mutex. Running lockdep shows that it is safe
    among the kernels I've run.
    
    Link: http://lkml.kernel.org/r/20120605102814.27845.21047.stgit@localhost.localdomain
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: "Frank Ch. Eigler" <fche@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/kprobes.c b/kernel/kprobes.c
index c62b8546cc90..7a8a1222c7b1 100644
--- a/kernel/kprobes.c
+++ b/kernel/kprobes.c
@@ -561,9 +561,9 @@ static __kprobes void kprobe_optimizer(struct work_struct *work)
 {
 	LIST_HEAD(free_list);
 
+	mutex_lock(&kprobe_mutex);
 	/* Lock modules while optimizing kprobes */
 	mutex_lock(&module_mutex);
-	mutex_lock(&kprobe_mutex);
 
 	/*
 	 * Step 1: Unoptimize kprobes and collect cleaned (unused and disarmed)
@@ -586,8 +586,8 @@ static __kprobes void kprobe_optimizer(struct work_struct *work)
 	/* Step 4: Free cleaned kprobes after quiesence period */
 	do_free_cleaned_kprobes(&free_list);
 
-	mutex_unlock(&kprobe_mutex);
 	mutex_unlock(&module_mutex);
+	mutex_unlock(&kprobe_mutex);
 
 	/* Step 5: Kick optimizer again if needed */
 	if (!list_empty(&optimizing_list) || !list_empty(&unoptimizing_list))

