commit a3bba370c227b6cbc480aa8959bb32cf966d79a6
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Tue Jan 27 16:52:50 2015 +0000

    arm64: drop unnecessary cache+tlb maintenance
    
    In paging_init, we call flush_cache_all, but this is backed by Set/Way
    operations which may not achieve anything in the presence of cache line
    migration and/or system caches. If the caches are already in an
    inconsistent state at this point, there is nothing we can do (short of
    flushing the entire physical address space by VA) to empty architected
    and system caches. As such, flush_cache_all only serves to mask other
    potential bugs. Hence, this patch removes the boot-time call to
    flush_cache_all.
    
    Immediately after the cache maintenance we flush the TLBs, but this is
    also unnecessary. Before enabling the MMU, the TLBs are invalidated, and
    thus are initially clean. When changing the contents of active tables
    (e.g. in fixup_executable() for DEBUG_RODATA) we perform the required
    TLB maintenance following the update, and therefore no additional
    maintenance is required to ensure the new table entries are in effect.
    Since activating the MMU we will not have modified system register
    fields permitted to be cached in a TLB, and therefore do not need
    maintenance for any cached system register fields. Hence, the TLB flush
    is unnecessary.
    
    Shortly after the unnecessary TLB flush, we update TTBR0 to point to an
    empty zero page rather than the idmap, and flush the TLBs. This
    maintenance is necessary to remove the global idmap entries from the
    TLBs (as they would conflict with userspace mappings), and is retained.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Marc Zyngier <marc.zyngier@arm.com>
    Acked-by: Steve Capper <steve.capper@linaro.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c
index eb293febfb56..a421f535d351 100644
--- a/arch/arm64/mm/mmu.c
+++ b/arch/arm64/mm/mmu.c
@@ -438,13 +438,6 @@ void __init paging_init(void)
 	map_mem();
 	fixup_executable();
 
-	/*
-	 * Finally flush the caches and tlb to ensure that we're in a
-	 * consistent state.
-	 */
-	flush_cache_all();
-	flush_tlb_all();
-
 	/* allocate the zero page. */
 	zero_page = early_alloc(PAGE_SIZE);
 

