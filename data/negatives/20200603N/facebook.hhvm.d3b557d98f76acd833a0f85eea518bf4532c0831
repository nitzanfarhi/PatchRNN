commit d3b557d98f76acd833a0f85eea518bf4532c0831
Author: mwilliams <mwilliams@fb.com>
Date:   Thu Jul 10 07:19:48 2014 -0700

    Updates to the relocation api
    
    Summary: More changes needed for live code relocation
    
    Reviewed By: @bertmaher
    
    Differential Revision: D1699527
    
    Signature: t1:1699527:1416864528:bf9653d5abed5fdeaf7f7d7a1fa4e193a5ff2bdd

diff --git a/hphp/runtime/vm/jit/back-end-x64.cpp b/hphp/runtime/vm/jit/back-end-x64.cpp
index 5c392de7c7..3d030438e5 100644
--- a/hphp/runtime/vm/jit/back-end-x64.cpp
+++ b/hphp/runtime/vm/jit/back-end-x64.cpp
@@ -94,8 +94,8 @@ struct BackEnd : public jit::BackEnd {
    * when we call it from C++, we have to tell gcc to clobber all the other
    * callee-saved registers.
    */
-  #define CALLEE_SAVED_BARRIER() \
-    asm volatile("" : : : "rbx", "r12", "r13", "r14", "r15")
+  #define CALLEE_SAVED_BARRIER()                                    \
+      asm volatile("" : : : "rbx", "r12", "r13", "r14", "r15");
 
   /*
    * enterTCHelper is a handwritten assembly function that transfers control in
@@ -237,9 +237,14 @@ struct BackEnd : public jit::BackEnd {
       case TestAndSmashFlags::kAlignJccAndJmp:
         // Ensure that the entire jcc, and the entire jmp are smashable
         // (but we dont need them both to be in the same cache line)
-        prepareForSmash(cb, testBytes + kJmpccLen, testBytes);
-        prepareForSmash(cb, testBytes + kJmpccLen + kJmpLen,
-                        testBytes + kJmpccLen);
+        prepareForSmashImpl(cb, testBytes + kJmpccLen, testBytes);
+        prepareForSmashImpl(cb, testBytes + kJmpccLen + kJmpLen,
+                            testBytes + kJmpccLen);
+        mcg->cgFixups().m_alignFixups.emplace(
+          cb.frontier(), std::make_pair(testBytes + kJmpccLen, testBytes));
+        mcg->cgFixups().m_alignFixups.emplace(
+          cb.frontier(), std::make_pair(testBytes + kJmpccLen + kJmpLen,
+                                        testBytes + kJmpccLen));
         assert(isSmashable(cb.frontier() + testBytes, kJmpccLen));
         assert(isSmashable(cb.frontier() + testBytes + kJmpccLen, kJmpLen));
         break;
@@ -256,11 +261,13 @@ struct BackEnd : public jit::BackEnd {
   size_t relocate(RelocationInfo& rel,
                   CodeBlock& destBlock,
                   TCA start, TCA end,
-                  CodeGenFixups& fixups) override {
+                  CodeGenFixups& fixups,
+                  TCA* exitAddr) override {
     WideJmpSet wideJmps;
     while (true) {
       try {
-        return relocateImpl(rel, destBlock, start, end, fixups, wideJmps);
+        return relocateImpl(rel, destBlock, start, end,
+                            fixups, exitAddr, wideJmps);
       } catch (JmpOutOfRange& j) {
       }
     }
@@ -270,6 +277,7 @@ struct BackEnd : public jit::BackEnd {
                       CodeBlock& destBlock,
                       TCA start, TCA end,
                       CodeGenFixups& fixups,
+                      TCA* exitAddr,
                       WideJmpSet& wideJmps) {
     TCA src = start;
     size_t range = end - src;
@@ -277,6 +285,9 @@ struct BackEnd : public jit::BackEnd {
     bool internalRefsNeedUpdating = false;
     TCA destStart = destBlock.frontier();
     size_t asm_count{0};
+    TCA jmpDest = nullptr;
+    TCA keepNopLow = nullptr;
+    TCA keepNopHigh = nullptr;
     try {
       while (src != end) {
         assert(src < end);
@@ -286,6 +297,11 @@ struct BackEnd : public jit::BackEnd {
         int destRange = 0;
         auto af = fixups.m_alignFixups.equal_range(src);
         while (af.first != af.second) {
+          auto low = src + af.first->second.second;
+          auto hi = src + af.first->second.first;
+          assert(low < hi);
+          if (!keepNopLow || keepNopLow > low) keepNopLow = low;
+          if (!keepNopHigh || keepNopHigh < hi) keepNopHigh = hi;
           TCA tmp = destBlock.frontier();
           prepareForSmashImpl(destBlock,
                               af.first->second.first, af.first->second.second);
@@ -296,20 +312,26 @@ struct BackEnd : public jit::BackEnd {
           ++af.first;
         }
 
+        bool preserveAlignment = keepNopLow && keepNopHigh &&
+          keepNopLow <= src && keepNopHigh > src;
+        TCA target = nullptr;
         TCA dest = destBlock.frontier();
         destBlock.bytes(di.size(), src);
         DecodedInstruction d2(dest);
         if (di.hasPicOffset()) {
+          if (di.isBranch(false)) {
+            target = di.picAddress();
+          }
           /*
            * Rip-relative offsets that point outside the range
            * being moved need to be adjusted so they continue
            * to point at the right thing
            */
-          if (size_t(di.picAddress() - start) > range) {
+          if (size_t(di.picAddress() - start) >= range) {
             bool DEBUG_ONLY success = d2.setPicAddress(di.picAddress());
             assert(success);
           } else {
-            if (d2.isBranch()) {
+            if (!preserveAlignment && d2.isBranch()) {
               if (wideJmps.count(src)) {
                 if (d2.size() < kJmpLen) {
                   d2.widenBranch();
@@ -324,7 +346,7 @@ struct BackEnd : public jit::BackEnd {
         }
         if (di.hasImmediate()) {
           if (fixups.m_addressImmediates.count(src)) {
-            if (size_t(di.immediate() - (uint64_t)start) <= range) {
+            if (size_t(di.immediate() - (uint64_t)start) < range) {
               hasInternalRefs = internalRefsNeedUpdating = true;
             }
           } else {
@@ -342,7 +364,7 @@ struct BackEnd : public jit::BackEnd {
              * fixups.m_addressImmediates. But it could just happen by bad
              * luck, so just log it.
              */
-            if (size_t(di.immediate() - (uint64_t)start) <= range) {
+            if (size_t(di.immediate() - (uint64_t)start) < range) {
               FTRACE(3,
                      "relocate: instruction at {} has immediate 0x{:x}"
                      "which looks like an address that needs relocating\n",
@@ -361,14 +383,26 @@ struct BackEnd : public jit::BackEnd {
         } else {
           rel.recordAddress(src, dest - destRange, destRange);
         }
-        if (di.isNop()) {
+        if (preserveAlignment && di.size() == kJmpLen &&
+            di.isNop() && src + kJmpLen == end) {
+          smashJmp(dest, src + kJmpLen);
+          dest += kJmpLen;
+        } else if (di.isNop() && !preserveAlignment) {
           internalRefsNeedUpdating = true;
         } else {
           dest += d2.size();
         }
+        jmpDest = target;
         assert(dest <= destBlock.frontier());
         destBlock.setFrontier(dest);
         src += di.size();
+        if (keepNopHigh && src >= keepNopHigh) {
+          keepNopLow = keepNopHigh = nullptr;
+        }
+      }
+
+      if (exitAddr) {
+        *exitAddr = jmpDest;
       }
 
       rel.recordRange(start, end, destStart, destBlock.frontier());
@@ -381,12 +415,12 @@ struct BackEnd : public jit::BackEnd {
           TCA newPicAddress = nullptr;
           int64_t newImmediate = 0;
           if (di.hasPicOffset() &&
-              size_t(di.picAddress() - start) <= range) {
+              size_t(di.picAddress() - start) < range) {
             newPicAddress = rel.adjustedAddressAfter(di.picAddress());
             always_assert(newPicAddress);
           }
           if (di.hasImmediate() &&
-              size_t((TCA)di.immediate() - start) <= range &&
+              size_t((TCA)di.immediate() - start) < range &&
               fixups.m_addressImmediates.count(src)) {
             newImmediate =
               (int64_t)rel.adjustedAddressAfter((TCA)di.immediate());
@@ -449,48 +483,72 @@ struct BackEnd : public jit::BackEnd {
   }
 
   void adjustForRelocation(RelocationInfo& rel) override {
-    for (const auto& range : rel) {
-      auto start = range.first;
-      auto end = range.second;
-      while (start != end) {
-        assert(start < end);
-        DecodedInstruction di(start);
+    for (const auto& range : rel.srcRanges()) {
+      adjustForRelocation(rel, range.first, range.second);
+    }
+  }
 
-        if (di.hasPicOffset()) {
-          /*
-           * A pointer into something that has been relocated needs to be
-           * updated.
-           */
-          if (TCA adjusted = rel.adjustedAddressAfter(di.picAddress())) {
-            di.setPicAddress(adjusted);
-          }
+  void adjustForRelocation(RelocationInfo& rel,
+                           TCA srcStart, TCA srcEnd) override {
+    auto start = rel.adjustedAddressAfter(srcStart);
+    auto end = rel.adjustedAddressBefore(srcEnd);
+    if (!start) {
+      start = srcStart;
+      end = srcEnd;
+    } else {
+      always_assert(end);
+    }
+    while (start != end) {
+      assert(start < end);
+      DecodedInstruction di(start);
+
+      if (di.hasPicOffset()) {
+        /*
+         * A pointer into something that has been relocated needs to be
+         * updated.
+         */
+        if (TCA adjusted = rel.adjustedAddressAfter(di.picAddress())) {
+          di.setPicAddress(adjusted);
         }
+      }
 
-        if (di.hasImmediate()) {
-          /*
-           * Similarly for addressImmediates - and see comment above
-           * for non-address immediates.
-           */
-          if (TCA adjusted = rel.adjustedAddressAfter((TCA)di.immediate())) {
-            if (rel.isAddressImmediate(start)) {
-              di.setImmediate((int64_t)adjusted);
-            } else {
-              FTRACE(3,
-                     "relocate: instruction at {} has immediate 0x{:x}"
-                     "which looks like an address that needs relocating\n",
-                     start, di.immediate());
-            }
+      if (di.hasImmediate()) {
+        /*
+         * Similarly for addressImmediates - and see comment above
+         * for non-address immediates.
+         */
+        if (TCA adjusted = rel.adjustedAddressAfter((TCA)di.immediate())) {
+          if (rel.isAddressImmediate(start)) {
+            di.setImmediate((int64_t)adjusted);
+          } else {
+            FTRACE(3,
+                   "relocate: instruction at {} has immediate 0x{:x}"
+                   "which looks like an address that needs relocating\n",
+                   start, di.immediate());
           }
         }
+      }
+
+      start += di.size();
+
+      if (start == end && di.isNop() &&
+          di.size() == kJmpLen &&
+          rel.adjustedAddressAfter(srcEnd)) {
 
-        start += di.size();
+        smashJmp(start - di.size(), rel.adjustedAddressAfter(end));
       }
     }
   }
 
-  void adjustForRelocation(RelocationInfo& rel,
-                           AsmInfo* asmInfo,
-                           CodeGenFixups& fixups) override {
+  /*
+   * Adjusts the addresses in asmInfo and fixups to match the new
+   * location of the code.
+   * This will not "hook up" the relocated code in any way, so is safe
+   * to call before the relocated code is ready to run.
+   */
+  void adjustMetaDataForRelocation(RelocationInfo& rel,
+                                   AsmInfo* asmInfo,
+                                   CodeGenFixups& fixups) override {
     auto& ip = fixups.m_inProgressTailJumps;
     for (size_t i = 0; i < ip.size(); ++i) {
       IncomingBranch& ib = const_cast<IncomingBranch&>(ip[i]);
@@ -533,21 +591,6 @@ struct BackEnd : public jit::BackEnd {
       }
     }
 
-    for (auto addr : fixups.m_reusedStubs) {
-      /*
-       * The stubs are terminated by a ud2. Check for it.
-       */
-      while (addr[0] != 0x0f || addr[1] != 0x0b) {
-        DecodedInstruction di(addr);
-        if (di.hasPicOffset()) {
-          if (TCA adjusted = rel.adjustedAddressAfter(di.picAddress())) {
-            di.setPicAddress(adjusted);
-          }
-        }
-        addr += di.size();
-      }
-    }
-
     /*
      * Most of the time we want to adjust to a corresponding "before" address
      * with the exception of the start of the range where "before" can point to
@@ -576,24 +619,28 @@ struct BackEnd : public jit::BackEnd {
       }
     }
 
-    std::set<TCA> updated;
+    decltype(fixups.m_addressImmediates) updatedAI;
     for (auto addrImm : fixups.m_addressImmediates) {
       if (TCA adjusted = rel.adjustedAddressAfter(addrImm)) {
-        updated.insert(adjusted);
+        updatedAI.insert(adjusted);
       } else if (TCA odd = rel.adjustedAddressAfter((TCA)~uintptr_t(addrImm))) {
         // just for cgLdObjMethod
-        updated.insert((TCA)~uintptr_t(odd));
+        updatedAI.insert((TCA)~uintptr_t(odd));
       } else {
-        updated.insert(addrImm);
+        updatedAI.insert(addrImm);
       }
     }
-    updated.swap(fixups.m_addressImmediates);
+    updatedAI.swap(fixups.m_addressImmediates);
 
-    for (auto codePtr : fixups.m_codePointers) {
-      if (TCA adjusted = rel.adjustedAddressAfter(*codePtr)) {
-        *codePtr = adjusted;
+    decltype(fixups.m_alignFixups) updatedAF;
+    for (auto af : fixups.m_alignFixups) {
+      if (TCA adjusted = rel.adjustedAddressAfter(af.first)) {
+        updatedAF.emplace(adjusted, af.second);
+      } else {
+        updatedAF.emplace(af);
       }
     }
+    updatedAF.swap(fixups.m_alignFixups);
 
     if (asmInfo) {
       fixupStateVector(asmInfo->asmInstRanges, rel);
@@ -605,6 +652,30 @@ struct BackEnd : public jit::BackEnd {
     }
   }
 
+  void adjustCodeForRelocation(RelocationInfo& rel,
+                               CodeGenFixups& fixups) override {
+    for (auto addr : fixups.m_reusedStubs) {
+      /*
+       * The stubs are terminated by a ud2. Check for it.
+       */
+      while (addr[0] != 0x0f || addr[1] != 0x0b) {
+        DecodedInstruction di(addr);
+        if (di.hasPicOffset()) {
+          if (TCA adjusted = rel.adjustedAddressAfter(di.picAddress())) {
+            di.setPicAddress(adjusted);
+          }
+        }
+        addr += di.size();
+      }
+    }
+
+    for (auto codePtr : fixups.m_codePointers) {
+      if (TCA adjusted = rel.adjustedAddressAfter(*codePtr)) {
+        *codePtr = adjusted;
+      }
+    }
+  }
+
  private:
   void smashJmpOrCall(TCA addr, TCA dest, bool isCall) {
     // Unconditional rip-relative jmps can also be encoded with an EB as the
@@ -680,7 +751,15 @@ struct BackEnd : public jit::BackEnd {
   }
 
   TCA jmpTarget(TCA jmp) override {
-    if (jmp[0] != 0xe9) return nullptr;
+    if (jmp[0] != 0xe9) {
+      if (jmp[0] == 0x0f &&
+          jmp[1] == 0x1f &&
+          jmp[2] == 0x44) {
+        // 5 byte nop
+        return jmp + 5;
+      }
+      return nullptr;
+    }
     return jmp + 5 + ((int32_t*)(jmp + 5))[-1];
   }
 
@@ -891,11 +970,11 @@ void BackEnd::genCodeImpl(IRUnit& unit, AsmInfo* asmInfo) {
     size_t asm_count{0};
     asm_count += be.relocate(rel, mainCodeIn,
                              mainCode.base(), mainCode.frontier(),
-                             mcg->cgFixups());
+                             mcg->cgFixups(), nullptr);
 
     asm_count += be.relocate(rel, coldCodeIn,
                              coldCode.base(), coldCode.frontier(),
-                             mcg->cgFixups());
+                             mcg->cgFixups(), nullptr);
     TRACE(1, "hhir-inst-count %ld asm %ld\n", hhir_count, asm_count);
 
     if (frozenCode != &coldCode) {
@@ -903,7 +982,8 @@ void BackEnd::genCodeImpl(IRUnit& unit, AsmInfo* asmInfo) {
                       frozenStart, frozenCode->frontier());
     }
     be.adjustForRelocation(rel);
-    be.adjustForRelocation(rel, asmInfo, mcg->cgFixups());
+    be.adjustMetaDataForRelocation(rel, asmInfo, mcg->cgFixups());
+    be.adjustCodeForRelocation(rel, mcg->cgFixups());
 
     if (asmInfo) {
       static int64_t mainDeltaTot = 0, coldDeltaTot = 0;
diff --git a/hphp/runtime/vm/jit/back-end.h b/hphp/runtime/vm/jit/back-end.h
index 75addcdfb0..59c3be7e9d 100644
--- a/hphp/runtime/vm/jit/back-end.h
+++ b/hphp/runtime/vm/jit/back-end.h
@@ -64,7 +64,7 @@ struct TReqInfo {
 enum class TestAndSmashFlags {
   kAlignJccImmediate,
   kAlignJcc,
-  kAlignJccAndJmp
+  kAlignJccAndJmp,
 };
 
 enum class MoveToAlignFlags {
@@ -128,6 +128,7 @@ class BackEnd {
   virtual void prepareForSmash(CodeBlock& cb, int nBytes, int offset = 0) = 0;
   virtual void prepareForTestAndSmash(CodeBlock& cb, int testBytes,
                                       TestAndSmashFlags flags) = 0;
+
   virtual void smashJmp(TCA jmpAddr, TCA newDest) = 0;
   virtual void smashCall(TCA callAddr, TCA newDest) = 0;
   virtual void smashJcc(TCA jccAddr, TCA newDest) = 0;
@@ -171,8 +172,9 @@ class BackEnd {
    * the same address as before relocation.
    */
   virtual size_t relocate(RelocationInfo& rel, CodeBlock& dest,
-                        TCA start, TCA end,
-                        CodeGenFixups& fixups) {
+                          TCA start, TCA end,
+                          CodeGenFixups& fixups,
+                          TCA* exitAddr) {
     always_assert(false);
     return 0;
   }
@@ -186,12 +188,39 @@ class BackEnd {
     always_assert(false);
   }
 
+  /*
+   * This will update a single range that was not relocated, but that
+   * might refer to relocated code (such as the cold code corresponding
+   * to a tracelet). Unless its guaranteed to be all position independent,
+   * its "fixups" should have been passed into a relocate call earlier.
+   */
+  virtual void adjustForRelocation(RelocationInfo& rel, TCA start, TCA end) {
+    always_assert(false);
+  }
+
   /*
    * Adjust the contents of fixups and asmInfo based on the relocation
-   * already performed on rel.
+   * already performed on rel. This will not cause any of the relocated
+   * code to be "hooked up", and its not safe to do so until all of the
+   * CodeGenFixups have been processed.
+   */
+  virtual void adjustMetaDataForRelocation(RelocationInfo& rel,
+                                           AsmInfo* asmInfo,
+                                           CodeGenFixups& fixups) {
+    always_assert(false);
+  }
+
+  /*
+   * Adjust potentially live references that point into the relocated
+   * area.
+   * Must not be called until its safe to run the relocated code.
    */
-  virtual void adjustForRelocation(RelocationInfo& rel,
-                                   AsmInfo* asmInfo, CodeGenFixups& fixups) {
+  virtual void adjustCodeForRelocation(RelocationInfo& rel,
+                                       CodeGenFixups& fixups) {
+    always_assert(false);
+  }
+
+  virtual void findFixups(TCA start, TCA end, CodeGenFixups& fixups) {
     always_assert(false);
   }
 };
diff --git a/hphp/runtime/vm/jit/fixup.h b/hphp/runtime/vm/jit/fixup.h
index 7ee8655c1e..18e3003ab5 100644
--- a/hphp/runtime/vm/jit/fixup.h
+++ b/hphp/runtime/vm/jit/fixup.h
@@ -145,6 +145,12 @@ public:
     m_fixups.insert(tca, FixupEntry(fixup));
   }
 
+  const Fixup* findFixup(CTCA tca) const {
+    auto ent = m_fixups.find(tca);
+    if (!ent) return nullptr;
+    return &ent->fixup;
+  }
+
   bool getFrameRegs(const ActRec* ar, const ActRec* prevAr,
                     VMRegs* outVMRegs) const;
 
diff --git a/hphp/runtime/vm/jit/mc-generator.cpp b/hphp/runtime/vm/jit/mc-generator.cpp
index 3fd2962286..d73bf066b9 100644
--- a/hphp/runtime/vm/jit/mc-generator.cpp
+++ b/hphp/runtime/vm/jit/mc-generator.cpp
@@ -467,15 +467,10 @@ MCGenerator::translate(const TranslArgs& args) {
   Func* func = const_cast<Func*>(args.m_sk.func());
   CodeCache::Selector cbSel(CodeCache::Selector::Args(code)
                             .profile(m_tx.mode() == TransKind::Profile)
-                            .hot((func->attrs() & AttrHot) && m_tx.useAHot()));
+                            .hot(RuntimeOption::EvalHotFuncCount &&
+                                 (func->attrs() & AttrHot) && m_tx.useAHot()));
 
-  if (args.m_align) {
-    mcg->backEnd().moveToAlign(code.main(),
-                               MoveToAlignFlags::kNonFallthroughAlign);
-  }
-
-  TCA start = code.main().frontier();
-  translateWork(args);
+  auto start = translateWork(args);
 
   if (args.m_setFuncBody) {
     func->setFuncBody(start);
@@ -631,14 +626,18 @@ MCGenerator::getFuncPrologue(Func* func, int nPassed, ActRec* ar,
 
   CodeCache::Selector cbSel(CodeCache::Selector::Args(code)
                             .profile(m_tx.mode() == TransKind::Proflogue)
-                            .hot((func->attrs() & AttrHot) && m_tx.useAHot()));
+                            .hot(RuntimeOption::EvalHotFuncCount &&
+                                 (func->attrs() & AttrHot) && m_tx.useAHot()));
 
+  assert(m_fixups.empty());
   // If we're close to a cache line boundary, just burn some space to
   // try to keep the func and its body on fewer total lines.
   if (((uintptr_t)code.main().frontier() & backEnd().cacheLineMask()) >=
       (backEnd().cacheLineSize() / 2)) {
     backEnd().moveToAlign(code.main(), MoveToAlignFlags::kCacheLineAlign);
   }
+  m_fixups.m_alignFixups.emplace(
+    code.main().frontier(), std::make_pair(backEnd().cacheLineSize() / 2, 0));
 
   // Careful: this isn't necessarily the real entry point. For funcIsMagic
   // prologues, this is just a possible prologue.
@@ -647,14 +646,9 @@ MCGenerator::getFuncPrologue(Func* func, int nPassed, ActRec* ar,
   TCA realColdStart   = mcg->code.realCold().frontier();
   TCA realFrozenStart = mcg->code.realFrozen().frontier();
 
-  auto const skFuncBody = [&] {
-    assert(m_fixups.empty());
-    auto ret = backEnd().emitFuncPrologue(code.main(), code.cold(), func,
-                                          funcIsMagic, nPassed,
-                                          start, aStart);
-    m_fixups.process(nullptr);
-    return ret;
-  }();
+  auto const skFuncBody = backEnd().emitFuncPrologue(
+    code.main(), code.cold(), func, funcIsMagic, nPassed, start, aStart);
+  m_fixups.process(nullptr);
 
   assert(backEnd().funcPrologueHasGuard(start, func));
   TRACE(2, "funcPrologue mcg %p %s(%d) setting prologue %p\n",
@@ -922,11 +916,13 @@ MCGenerator::bindJmpccFirst(TCA toSmash,
   }
 
   TCA stub = emitEphemeralServiceReq(code.frozen(),
-                                     getFreeStub(code.frozen(), nullptr),
+                                     getFreeStub(code.frozen(),
+                                                 &mcg->cgFixups()),
                                      REQ_BIND_JMPCC_SECOND,
                                      RipRelative(toSmash),
                                      offWillDefer, cc);
 
+  mcg->cgFixups().process(nullptr);
   smashed = true;
   assert(Translator::WriteLease().amOwner());
   /*
@@ -1381,14 +1377,14 @@ TCA MCGenerator::getFreeStub(CodeBlock& frozen, CodeGenFixups* fixups) {
     always_assert(m_freeStubs.m_list == nullptr ||
                   code.isValidCodeAddress(TCA(m_freeStubs.m_list)));
     TRACE(1, "recycle stub %p\n", ret);
-    if (fixups) {
-      fixups->m_reusedStubs.emplace_back(ret);
-    }
   } else {
     ret = frozen.frontier();
     Stats::inc(Stats::Astub_New);
     TRACE(1, "alloc new stub %p\n", ret);
   }
+  if (fixups) {
+    fixups->m_reusedStubs.emplace_back(ret);
+  }
   return ret;
 }
 
@@ -1530,7 +1526,8 @@ template <typename T> void ClearContainer(T& container) {
 }
 
 void
-CodeGenFixups::process(GrowableVector<IncomingBranch>* inProgressTailBranches) {
+CodeGenFixups::process_only(
+  GrowableVector<IncomingBranch>* inProgressTailBranches) {
   for (uint i = 0; i < m_pendingFixups.size(); i++) {
     TCA tca = m_pendingFixups[i].m_tca;
     assert(mcg->isValidCodeAddress(tca));
@@ -1555,21 +1552,6 @@ CodeGenFixups::process(GrowableVector<IncomingBranch>* inProgressTailBranches) {
     m_inProgressTailJumps.swap(*inProgressTailBranches);
   }
   assert(m_inProgressTailJumps.empty());
-
-  /*
-   * Currently these are only used by the relocator,
-   * so there's nothing left to do here.
-   *
-   * Once we try to relocate live code, we'll need to
-   * store compact forms of these for later.
-   */
-  ClearContainer(m_reusedStubs);
-  ClearContainer(m_addressImmediates);
-  ClearContainer(m_codePointers);
-  ClearContainer(m_bcMap);
-  ClearContainer(m_alignFixups);
-
-  assert(empty());
 }
 
 void CodeGenFixups::clear() {
@@ -1599,7 +1581,7 @@ bool CodeGenFixups::empty() const {
     m_literals.empty();
 }
 
-void
+TCA
 MCGenerator::translateWork(const TranslArgs& args) {
   Timer _t(Timer::translate);
   auto sk = args.m_sk;
@@ -1607,6 +1589,11 @@ MCGenerator::translateWork(const TranslArgs& args) {
   SKTRACE(1, sk, "translateWork\n");
   assert(m_tx.getSrcDB().find(sk));
 
+  if (args.m_align) {
+    mcg->backEnd().moveToAlign(code.main(),
+                               MoveToAlignFlags::kNonFallthroughAlign);
+  }
+
   TCA        start             = code.main().frontier();
   TCA        coldStart         = code.cold().frontier();
   TCA        realColdStart     = code.realCold().frontier();
@@ -1752,7 +1739,7 @@ MCGenerator::translateWork(const TranslArgs& args) {
 
   if (args.m_dryRun) {
     resetState();
-    return;
+    return start;
   }
 
   if (transKindToRecord == TransKind::Interp) {
@@ -1762,6 +1749,11 @@ MCGenerator::translateWork(const TranslArgs& args) {
     // Fall through.
   }
 
+  if (args.m_align) {
+    m_fixups.m_alignFixups.emplace(
+      start, std::make_pair(backEnd().cacheLineSize() - 1, 0));
+  }
+
   if (RuntimeOption::EvalProfileBC) {
     auto* unit = sk.unit();
     TransBCMapping prev{};
@@ -1816,6 +1808,8 @@ MCGenerator::translateWork(const TranslArgs& args) {
   if (Trace::moduleEnabledRelease(Trace::tcspace, 1)) {
     Trace::traceRelease("%s", getUsageString().c_str());
   }
+
+  return start;
 }
 
 void MCGenerator::traceCodeGen() {
diff --git a/hphp/runtime/vm/jit/mc-generator.h b/hphp/runtime/vm/jit/mc-generator.h
index ab4e59cb8b..8e013abd91 100644
--- a/hphp/runtime/vm/jit/mc-generator.h
+++ b/hphp/runtime/vm/jit/mc-generator.h
@@ -106,7 +106,11 @@ struct CodeGenFixups {
     m_tletFrozen = frozen;
   }
 
-  void process(GrowableVector<IncomingBranch>* inProgressTailBranches);
+  void process_only(GrowableVector<IncomingBranch>* inProgressTailBranches);
+  void process(GrowableVector<IncomingBranch>* inProgressTailBranches) {
+    process_only(inProgressTailBranches);
+    clear();
+  }
   bool empty() const;
   void clear();
 };
@@ -126,15 +130,15 @@ struct RelocationInfo {
     return adjustedAddressBefore(const_cast<TCA>(addr));
   }
   void rewind(TCA start, TCA end);
-  void markAddressImmediates(std::set<TCA> ai) {
+  void markAddressImmediates(const std::set<TCA>& ai) {
     m_addressImmediates.insert(ai.begin(), ai.end());
   }
   bool isAddressImmediate(TCA ip) {
     return m_addressImmediates.count(ip);
   }
   typedef std::vector<std::pair<TCA,TCA>> RangeVec;
-  RangeVec::iterator begin() { return m_dstRanges.begin(); }
-  RangeVec::iterator end() { return m_dstRanges.end(); }
+  const RangeVec& srcRanges() { return m_srcRanges; }
+  const RangeVec& dstRanges() { return m_dstRanges; }
  private:
   RangeVec m_srcRanges;
   RangeVec m_dstRanges;
@@ -343,7 +347,7 @@ private:
   TCA createTranslation(const TranslArgs& args);
   TCA retranslate(const TranslArgs& args);
   TCA translate(const TranslArgs& args);
-  void translateWork(const TranslArgs& args);
+  TCA translateWork(const TranslArgs& args);
 
   TCA lookupTranslation(SrcKey sk) const;
   TCA retranslateOpt(TransID transId, bool align);
diff --git a/hphp/runtime/vm/jit/srcdb.cpp b/hphp/runtime/vm/jit/srcdb.cpp
index 32d3c4bd0e..28ca13c6fc 100644
--- a/hphp/runtime/vm/jit/srcdb.cpp
+++ b/hphp/runtime/vm/jit/srcdb.cpp
@@ -29,6 +29,60 @@ namespace HPHP { namespace jit {
 
 TRACE_SET_MOD(trans)
 
+void IncomingBranch::relocate(RelocationInfo& rel) {
+  // compute adjustedTarget before altering the smash address,
+  // because it might be a 5-byte nop
+  TCA adjustedTarget = rel.adjustedAddressAfter(target());
+
+  if (TCA adjusted = rel.adjustedAddressAfter(toSmash())) {
+    m_ptr.set(m_ptr.tag(), adjusted);
+  }
+
+  if (adjustedTarget) {
+    FTRACE_MOD(Trace::mcg, 1, "Patching: 0x{:08x} from 0x{:08x} to 0x{:08x}\n",
+               (uintptr_t)toSmash(), (uintptr_t)target(),
+               (uintptr_t)adjustedTarget);
+
+    patch(adjustedTarget);
+  }
+}
+
+void IncomingBranch::patch(TCA dest) {
+  switch (type()) {
+    case Tag::JMP: {
+      mcg->backEnd().smashJmp(toSmash(), dest);
+      break;
+    }
+
+    case Tag::JCC: {
+      mcg->backEnd().smashJcc(toSmash(), dest);
+      break;
+    }
+
+    case Tag::ADDR: {
+      // Note that this effectively ignores a
+      TCA* addr = reinterpret_cast<TCA*>(toSmash());
+      assert_address_is_atomically_accessible(addr);
+      *addr = dest;
+      break;
+    }
+  }
+}
+
+TCA IncomingBranch::target() const {
+  switch (type()) {
+    case Tag::JMP:
+      return mcg->backEnd().jmpTarget(toSmash());
+
+    case Tag::JCC:
+      return mcg->backEnd().jccTarget(toSmash());
+
+    case Tag::ADDR:
+      return *reinterpret_cast<TCA*>(toSmash());
+  }
+  always_assert(false);
+}
+
 void SrcRec::setFuncInfo(const Func* f) {
   m_unitMd5 = f->unit()->md5();
 }
@@ -56,7 +110,7 @@ void SrcRec::chainFrom(IncomingBranch br) {
   TRACE(1, "SrcRec(%p)::chainFrom %p -> %p (type %d); %zd incoming branches\n",
         this,
         br.toSmash(), destAddr, br.type(), m_incomingBranches.size());
-  patch(br, destAddr);
+  br.patch(destAddr);
 }
 
 void SrcRec::emitFallbackJump(CodeBlock& cb, ConditionCode cc /* = -1 */) {
@@ -118,8 +172,8 @@ void SrcRec::newTranslation(TCA newStart,
    * get into REQ_RETRANSLATE, they'll acquire it and generate a
    * translation possibly for this same situation.)
    */
-  for (size_t i = 0; i < m_tailFallbackJumps.size(); ++i) {
-    patch(m_tailFallbackJumps[i], newStart);
+  for (auto& br : m_tailFallbackJumps) {
+    br.patch(newStart);
   }
 
   // This is the new tail translation, so store the fallback jump list
@@ -127,6 +181,30 @@ void SrcRec::newTranslation(TCA newStart,
   m_tailFallbackJumps.swap(tailBranches);
 }
 
+void SrcRec::relocate(RelocationInfo& rel) {
+  if (auto adjusted = rel.adjustedAddressAfter(m_anchorTranslation)) {
+    m_anchorTranslation = adjusted;
+  }
+
+  if (auto adjusted = rel.adjustedAddressAfter(m_topTranslation.load())) {
+    m_topTranslation.store(adjusted);
+  }
+
+  for (auto &t : m_translations) {
+    if (TCA adjusted = rel.adjustedAddressAfter(t)) {
+      t = adjusted;
+    }
+  }
+
+  for (auto &ib : m_tailFallbackJumps) {
+    ib.relocate(rel);
+  }
+
+  for (auto &ib : m_incomingBranches) {
+    ib.relocate(rel);
+  }
+}
+
 void SrcRec::addDebuggerGuard(TCA dbgGuard, TCA dbgBranchGuardSrc) {
   assert(!m_dbgBranchGuardSrc);
 
@@ -153,11 +231,10 @@ void SrcRec::patchIncomingBranches(TCA newStart) {
 
   TRACE(1, "%zd incoming branches to rechain\n", m_incomingBranches.size());
 
-  auto& change = m_incomingBranches;
-  for (unsigned i = 0; i < change.size(); ++i) {
+  for (auto &br : m_incomingBranches) {
     TRACE(1, "SrcRec(%p)::newTranslation rechaining @%p -> %p\n",
-          this, change[i].toSmash(), newStart);
-    patch(change[i], newStart);
+          this, br.toSmash(), newStart);
+    br.patch(newStart);
   }
 }
 
@@ -190,26 +267,4 @@ void SrcRec::replaceOldTranslations() {
   patchIncomingBranches(m_anchorTranslation);
 }
 
-void SrcRec::patch(IncomingBranch branch, TCA dest) {
-  switch (branch.type()) {
-  case IncomingBranch::Tag::JMP: {
-    mcg->backEnd().smashJmp(branch.toSmash(), dest);
-    break;
-  }
-
-  case IncomingBranch::Tag::JCC: {
-    mcg->backEnd().smashJcc(branch.toSmash(), dest);
-    break;
-  }
-
-  case IncomingBranch::Tag::ADDR: {
-    // Note that this effectively ignores a
-    TCA* addr = reinterpret_cast<TCA*>(branch.toSmash());
-    assert_address_is_atomically_accessible(addr);
-    *addr = dest;
-    break;
-  }
-  }
-}
-
 } } // HPHP::jit
diff --git a/hphp/runtime/vm/jit/srcdb.h b/hphp/runtime/vm/jit/srcdb.h
index 4b5562e2d2..99578dbdfa 100644
--- a/hphp/runtime/vm/jit/srcdb.h
+++ b/hphp/runtime/vm/jit/srcdb.h
@@ -30,10 +30,19 @@
 namespace HPHP { namespace jit {
 
 struct CodeGenFixups;
+struct RelocationInfo;
 
 template<typename T>
 struct GrowableVector {
   GrowableVector() {}
+  GrowableVector(GrowableVector&& other) noexcept : m_vec(other.m_vec) {
+    other.m_vec = nullptr;
+  }
+  GrowableVector& operator=(GrowableVector&& other) {
+    m_vec = other.m_vec;
+    other.m_vec = nullptr;
+    return *this;
+  }
   GrowableVector(const GrowableVector&) = delete;
   GrowableVector& operator=(const GrowableVector&) = delete;
 
@@ -128,10 +137,12 @@ struct IncomingBranch {
 
   Tag type()        const { return m_ptr.tag(); }
   TCA toSmash()     const { return TCA(m_ptr.ptr()); }
+  void relocate(RelocationInfo& rel);
   void adjust(TCA addr) {
     m_ptr.set(m_ptr.tag(), addr);
   }
-
+  void patch(TCA dest);
+  TCA target() const;
 private:
   explicit IncomingBranch(Tag type, TCA toSmash) {
     m_ptr.set(type, toSmash);
@@ -188,6 +199,10 @@ struct SrcRec {
     return m_translations;
   }
 
+  const GrowableVector<IncomingBranch>& tailFallbackJumps() {
+    return m_tailFallbackJumps;
+  }
+
   /*
    * The anchor translation is a retranslate request for the current
    * SrcKey that will continue the tracelet chain.
@@ -202,6 +217,8 @@ struct SrcRec {
     return m_incomingBranches;
   }
 
+  void relocate(RelocationInfo& rel);
+
   /*
    * There is an unlikely race in retranslate, where two threads
    * could simultaneously generate the same translation for a
@@ -222,13 +239,12 @@ struct SrcRec {
 
 private:
   TCA getFallbackTranslation() const;
-  void patch(IncomingBranch branch, TCA dest);
   void patchIncomingBranches(TCA newStart);
 
 private:
   // This either points to the most recent translation in the
   // translations vector, or if hasDebuggerGuard() it points to the
-  // debug guard.  Read/write with atomic primitives only.
+  // debug guard.
   std::atomic<TCA> m_topTranslation;
 
   /*
diff --git a/hphp/runtime/vm/jit/unique-stubs-x64.cpp b/hphp/runtime/vm/jit/unique-stubs-x64.cpp
index 8e6b87b435..919e3dbf96 100644
--- a/hphp/runtime/vm/jit/unique-stubs-x64.cpp
+++ b/hphp/runtime/vm/jit/unique-stubs-x64.cpp
@@ -152,8 +152,10 @@ void emitFreeLocalsHelpers(UniqueStubs& uniqueStubs) {
   auto const rData     = rdi;
   int const tvSize     = sizeof(TypedValue);
 
-  Asm a { mcg->code.main() };
-  moveToAlign(mcg->code.main(), kNonFallthroughAlign);
+  auto& cb = mcg->code.hot().available() > 512 ?
+    const_cast<CodeBlock&>(mcg->code.hot()) : mcg->code.main();
+  Asm a { cb };
+  moveToAlign(cb, kNonFallthroughAlign);
   auto stubBegin = a.frontier();
 
 asm_label(a, release);
@@ -167,7 +169,7 @@ asm_label(a, release);
 asm_label(a, doRelease);
   a.    jmp    (lookupDestructor(a, PhysReg(rType)));
 
-  moveToAlign(mcg->code.main(), kJmpTargetAlign);
+  moveToAlign(cb, kJmpTargetAlign);
   uniqueStubs.freeManyLocalsHelper = a.frontier();
   a.    lea    (rVmFp[-(jit::kNumFreeLocalsHelpers * sizeof(Cell))],
                 rFinished);
@@ -207,9 +209,11 @@ asm_label(a, loopHead);
 }
 
 void emitFuncPrologueRedispatch(UniqueStubs& uniqueStubs) {
-  Asm a { mcg->code.main() };
+  auto& cb = mcg->code.hot().available() > 512 ?
+    const_cast<CodeBlock&>(mcg->code.hot()) : mcg->code.main();
+  Asm a { cb };
 
-  moveToAlign(mcg->code.main());
+  moveToAlign(cb);
   uniqueStubs.funcPrologueRedispatch = a.frontier();
 
   assert(kScratchCrossTraceRegs.contains(rax));
@@ -255,9 +259,11 @@ asm_label(a, numParamsCheck);
 }
 
 void emitFCallArrayHelper(UniqueStubs& uniqueStubs) {
-  Asm a { mcg->code.main() };
+  auto& cb = mcg->code.hot().available() > 512 ?
+    const_cast<CodeBlock&>(mcg->code.hot()) : mcg->code.main();
+  Asm a { cb };
 
-  moveToAlign(mcg->code.main(), kNonFallthroughAlign);
+  moveToAlign(cb, kNonFallthroughAlign);
   uniqueStubs.fcallArrayHelper = a.frontier();
 
   /*
diff --git a/hphp/runtime/vm/jit/unique-stubs.cpp b/hphp/runtime/vm/jit/unique-stubs.cpp
index 782d4463e9..fd308f0bc1 100644
--- a/hphp/runtime/vm/jit/unique-stubs.cpp
+++ b/hphp/runtime/vm/jit/unique-stubs.cpp
@@ -27,30 +27,25 @@ TRACE_SET_MOD(ustubs);
 //////////////////////////////////////////////////////////////////////
 
 TCA UniqueStubs::add(const char* name, TCA start) {
-  auto const inACold = start > mcg->code.cold().base();
-  UNUSED auto const stop = inACold ? mcg->code.cold().frontier()
-    : mcg->code.main().frontier();
+  auto& cb = mcg->code.blockFor(start);
 
-  FTRACE(1, "unique stub: {} {} -- {:4} bytes: {}\n",
-         inACold ? "acold @ "
-         : "  ahot @  ",
+  FTRACE(1, "unique stub: {} @ {} -- {:4} bytes: {}\n",
+         cb.name(),
          static_cast<void*>(start),
-         static_cast<size_t>(stop - start),
+         static_cast<size_t>(cb.frontier() - start),
          name);
 
   ONTRACE(2,
           [&]{
             Disasm dasm(Disasm::Options().indent(4));
             std::ostringstream os;
-            dasm.disasm(os, start, stop);
+            dasm.disasm(os, start, cb.frontier());
             FTRACE(2, "{}\n", os.str());
           }()
          );
 
   mcg->recordGdbStub(
-    mcg->code.blockFor(start),
-    start,
-    strdup(folly::format("HHVM::{}", name).str().c_str())
+    cb, start, strdup(folly::sformat("HHVM::{}", name).c_str())
   );
   return start;
 }
diff --git a/hphp/runtime/vm/jit/vasm-x64.cpp b/hphp/runtime/vm/jit/vasm-x64.cpp
index e9aba74d43..3ab8d78dda 100644
--- a/hphp/runtime/vm/jit/vasm-x64.cpp
+++ b/hphp/runtime/vm/jit/vasm-x64.cpp
@@ -649,8 +649,7 @@ void Vgen::emit(jit::vector<Vlabel>& labels) {
 
   // This is under the printir tracemod because it mostly shows you IR and
   // machine code, not vasm and machine code (not implemented).
-  bool shouldUpdateAsmInfo = !!m_asmInfo
-    && Trace::moduleEnabledRelease(HPHP::Trace::printir, kCodeGenLevel);
+  bool shouldUpdateAsmInfo = !!m_asmInfo;
 
   std::vector<TransBCMapping>* bcmap = nullptr;
   if (mcg->tx().isTransDBEnabled() || RuntimeOption::EvalJitUseVtuneAPI) {
diff --git a/hphp/runtime/vm/type-profile.h b/hphp/runtime/vm/type-profile.h
index 6006b0bc9f..6abc8133e8 100644
--- a/hphp/runtime/vm/type-profile.h
+++ b/hphp/runtime/vm/type-profile.h
@@ -72,6 +72,7 @@ inline bool isStandardRequest() {
   return standardRequest;
 }
 
+void setRelocateRequests(int32_t n);
 //////////////////////////////////////////////////////////////////////
 
 }
diff --git a/hphp/util/asm-x64.cpp b/hphp/util/asm-x64.cpp
index 7965671bb2..5447b43098 100644
--- a/hphp/util/asm-x64.cpp
+++ b/hphp/util/asm-x64.cpp
@@ -353,7 +353,7 @@ std::string DecodedInstruction::toString() {
                            (uint64_t)m_ip,
                            m_opcode).str();
   if (m_flags.hasModRm) {
-    auto modRm = m_ip[m_size - m_immSz - m_offSz - m_flags.hasSib - 1];
+    auto modRm = getModRm();
     str += folly::format(" ModRM({:02b} {} {})",
                          modRm >> 6,
                          (modRm >> 3) & 7,
@@ -418,16 +418,16 @@ bool DecodedInstruction::isNop() const {
   return m_opcode == 0x1f && m_map_select == 1;
 }
 
-bool DecodedInstruction::isBranch() const {
+bool DecodedInstruction::isBranch(bool allowCond /* = true */) const {
   if (!m_flags.picOff) return false;
   if (m_map_select == 0) {
     // The one-byte opcode map
     return
-      (m_opcode & 0xf0) == 0x70 /* 8-bit conditional branch */ ||
+      ((m_opcode & 0xf0) == 0x70 && allowCond) /* 8-bit conditional branch */ ||
       m_opcode == 0xe9 /* 32-bit unconditional branch */ ||
       m_opcode == 0xeb /* 8-bit unconditional branch */;
   }
-  if (m_map_select == 1) {
+  if (m_map_select == 1 && allowCond) {
     // The two-byte opcode map (first byte is 0x0f)
     return (m_opcode & 0xf0) == 0x80 /* 32-bit conditional branch */;
   }
diff --git a/hphp/util/asm-x64.h b/hphp/util/asm-x64.h
index b6e9a9258a..c02e5d970f 100644
--- a/hphp/util/asm-x64.h
+++ b/hphp/util/asm-x64.h
@@ -2230,7 +2230,7 @@ struct DecodedInstruction {
   int64_t immediate() const;
   bool setImmediate(int64_t value);
   bool isNop() const;
-  bool isBranch() const;
+  bool isBranch(bool allowCond = true) const;
   bool isCall() const;
   bool shrinkBranch();
   void widenBranch();

