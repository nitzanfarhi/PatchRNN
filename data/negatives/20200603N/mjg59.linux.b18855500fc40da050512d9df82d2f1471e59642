commit b18855500fc40da050512d9df82d2f1471e59642
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Sun Sep 15 17:49:13 2013 +0400

    sched/balancing: Fix 'local->avg_load > sds->avg_load' case in calculate_imbalance()
    
    In busiest->group_imb case we can come to calculate_imbalance() with
    local->avg_load >= busiest->avg_load >= sds->avg_load. This can result
    in imbalance overflow, because it is calculated as follows
    
    env->imbalance = min(
            max_pull * busiest->group_power,
            (sds->avg_load - local->avg_load) * local->group_power) / SCHED_POWER_SCALE;
    
    As a result we can end up constantly bouncing tasks from one cpu to
    another if there are pinned tasks.
    
    Fix this by skipping the assignment and assuming imbalance=0 in case
    local->avg_load > sds->avg_load.
    
    [ The bug can be caught by running 2*N cpuhogs pinned to two logical cpus
      belonging to different cores on an HT-enabled machine with N logical
      cpus: just look at se.nr_migrations growth. ]
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/8f596cc6bc0e5e655119dc892c9bfcad26e971f4.1379252740.git.vdavydov@parallels.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index 11cd13667359..0b99aae339cb 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -4896,7 +4896,8 @@ static inline void calculate_imbalance(struct lb_env *env, struct sd_lb_stats *s
 	 * max load less than avg load(as we skip the groups at or below
 	 * its cpu_power, while calculating max_load..)
 	 */
-	if (busiest->avg_load < sds->avg_load) {
+	if (busiest->avg_load <= sds->avg_load ||
+	    local->avg_load >= sds->avg_load) {
 		env->imbalance = 0;
 		return fix_small_imbalance(env, sds);
 	}

