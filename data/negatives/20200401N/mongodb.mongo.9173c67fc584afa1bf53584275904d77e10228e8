commit 9173c67fc584afa1bf53584275904d77e10228e8
Author: Hari Khalsa <hkhalsa@10gen.com>
Date:   Thu May 22 11:29:06 2014 -0400

    SERVER-13641 remove dead index stats and storage details cmds

diff --git a/src/mongo/db/commands/index_stats.cpp b/src/mongo/db/commands/index_stats.cpp
deleted file mode 100644
index 9b17a0179b..0000000000
--- a/src/mongo/db/commands/index_stats.cpp
+++ /dev/null
@@ -1,568 +0,0 @@
-/**
- * collection.indexStats({...}) command
- */
-
-/*    Copyright 2012 10gen Inc.
- *
- *    This program is free software: you can redistribute it and/or  modify
- *    it under the terms of the GNU Affero General Public License, version 3,
- *    as published by the Free Software Foundation.
- *
- *    This program is distributed in the hope that it will be useful,
- *    but WITHOUT ANY WARRANTY; without even the implied warranty of
- *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- *    GNU Affero General Public License for more details.
- *
- *    You should have received a copy of the GNU Affero General Public License
- *    along with this program.  If not, see <http://www.gnu.org/licenses/>.
- *
- *    As a special exception, the copyright holders give permission to link the
- *    code of portions of this program with the OpenSSL library under certain
- *    conditions as described in each individual source file and distribute
- *    linked combinations including the program with the OpenSSL library. You
- *    must comply with the GNU Affero General Public License in all respects
- *    for all of the code used other than as permitted herein. If you modify
- *    file(s) with this exception, you may extend this exception to your
- *    version of the file(s), but you are not obligated to do so. If you do not
- *    wish to do so, delete this exception statement from your version. If you
- *    delete this exception statement from all source files in the program,
- *    then also delete it in the license file.
- */
-
-#if 0 // disabled pending SERVER-13555
-
-#include "mongo/pch.h"
-
-#include <iostream>
-#include <string>
-#include <vector>
-
-#include <boost/noncopyable.hpp>
-#include <boost/optional.hpp>
-
-#include "mongo/base/init.h"
-#include "mongo/db/auth/action_set.h"
-#include "mongo/db/auth/action_type.h"
-#include "mongo/db/auth/privilege.h"
-#include "mongo/db/catalog/index_catalog.h"
-#include "mongo/db/catalog/index_catalog_entry.h"
-#include "mongo/db/commands.h"
-#include "mongo/db/db.h"
-#include "mongo/db/jsobj.h"
-#include "mongo/db/index/index_descriptor.h"
-#include "mongo/db/kill_current_op.h"
-#include "mongo/db/structure/btree/btree.h"
-#include "mongo/util/descriptive_stats.h"
-
-namespace mongo {
-
-    /**
-     * Holds operation parameters.
-     */
-    struct IndexStatsParams {
-        string indexName;
-        vector<int> expandNodes;
-    };
-
-    /**
-     * Holds information about a single btree bucket (not its subtree).
-     */
-    struct NodeInfo {
-        NodeInfo() : childNum(0), keyCount(0), usedKeyCount(0), depth(0), fillRatio(0) {}
-
-        boost::optional<BSONObj> firstKey;
-        boost::optional<BSONObj> lastKey;
-        BSONObj diskLoc;
-        unsigned int childNum;
-        unsigned int keyCount;
-        unsigned int usedKeyCount;
-        unsigned int depth;
-        double fillRatio;
-    };
-
-    /**
-     * Aggregates and statistics for some part of the tree:
-     *     the entire tree, a level or a certain subtree.
-     */
-    class AreaStats {
-    public:
-        static const int quantiles = 99;
-
-        boost::optional<NodeInfo> nodeInfo;
-
-        unsigned int numBuckets;
-        SummaryEstimators<double, quantiles> bsonRatio;
-        SummaryEstimators<double, quantiles> fillRatio;
-        SummaryEstimators<double, quantiles> keyNodeRatio;
-        SummaryEstimators<unsigned int, quantiles> keyCount;
-        SummaryEstimators<unsigned int, quantiles> usedKeyCount;
-
-        AreaStats() : numBuckets(0) {
-        }
-
-        virtual ~AreaStats() {
-        }
-
-        /**
-         * add the provided values as a sample to the computed statistics for this tree / level /
-         * subtree
-         *
-         * @param keyCount number of keys in the bucket
-         * @param usedKeyCount number of used (non-empty) keys in the bucket
-         * @param bucket current bucket
-         * @param keyNodeBytes size (number of bytes) of a KeyNode
-         */
-        template<class Version>
-        void addStats(int keyCount, int usedKeyCount, const BtreeBucket<Version>* bucket,
-                      int keyNodeBytes) {
-            this->numBuckets++;
-            this->keyCount << keyCount;
-            this->usedKeyCount << usedKeyCount;
-            this->bsonRatio << (static_cast<double>(bucket->getTopSize()) / bucket->bodySize());
-            this->keyNodeRatio <<
-                    (static_cast<double>(keyNodeBytes * keyCount) / bucket->bodySize());
-            this->fillRatio <<
-                    (1.0 - static_cast<double>(bucket->getEmptySize()) / bucket->bodySize());
-        }
-
-        void appendTo(BSONObjBuilder& builder) const {
-            if (nodeInfo) {
-                BSONObjBuilder nodeInfoBuilder(builder.subobjStart("nodeInfo"));
-                nodeInfoBuilder << "childNum" << nodeInfo->childNum
-                                << "keyCount" << nodeInfo->keyCount
-                                << "usedKeyCount" << nodeInfo->usedKeyCount
-                                << "diskLoc" << nodeInfo->diskLoc
-                                << "depth" << nodeInfo->depth
-                                << "fillRatio" << nodeInfo->fillRatio;
-                if (nodeInfo->firstKey) nodeInfoBuilder << "firstKey" << *(nodeInfo->firstKey);
-                if (nodeInfo->lastKey) nodeInfoBuilder << "lastKey" << *(nodeInfo->lastKey);
-            }
-
-            builder << "numBuckets" << numBuckets
-                    << "keyCount" << keyCount.statisticSummaryToBSONObj()
-                    << "usedKeyCount" << usedKeyCount.statisticSummaryToBSONObj()
-                    << "bsonRatio" << bsonRatio.statisticSummaryToBSONObj()
-                    << "keyNodeRatio" << keyNodeRatio.statisticSummaryToBSONObj()
-                    << "fillRatio" << fillRatio.statisticSummaryToBSONObj();
-        }
-    };
-
-    /**
-     * Holds statistics and aggregates for the entire tree and its parts.
-     */
-    class BtreeStats {
-    public:
-        unsigned int bucketBodyBytes;
-        unsigned int depth;
-        AreaStats wholeTree;
-        vector<AreaStats> perLevel;
-        vector<vector<AreaStats> > branch;
-
-        BtreeStats() : bucketBodyBytes(0), depth(0) {
-            branch.push_back(vector<AreaStats>(1));
-        }
-
-        AreaStats& nodeAt(unsigned int nodeDepth, unsigned int childNum) {
-            verify(branch.size() > nodeDepth);
-            verify(branch[nodeDepth].size() > childNum);
-            return branch[nodeDepth][childNum];
-        }
-
-        void newBranchLevel(unsigned int childrenCount) {
-            branch.push_back(vector<AreaStats>(childrenCount));
-        }
-
-        void appendTo(BSONObjBuilder& builder) const {
-            builder << "bucketBodyBytes" << bucketBodyBytes;
-            builder << "depth" << depth;
-
-            BSONObjBuilder wholeTreeBuilder(builder.subobjStart("overall"));
-            wholeTree.appendTo(wholeTreeBuilder);
-            wholeTreeBuilder.doneFast();
-
-            BSONArrayBuilder perLevelArrayBuilder(builder.subarrayStart("perLevel"));
-            for (vector<AreaStats>::const_iterator it = perLevel.begin();
-                 it != perLevel.end();
-                 ++it) {
-                BSONObjBuilder levelBuilder(perLevelArrayBuilder.subobjStart());
-                it->appendTo(levelBuilder);
-                levelBuilder.doneFast();
-            }
-            perLevelArrayBuilder.doneFast();
-
-            if (branch.size() > 1) {
-                BSONArrayBuilder expandedNodesArrayBuilder(builder.subarrayStart("expandedNodes"));
-                for (unsigned int depth = 0; depth < branch.size(); ++depth) {
-
-                    BSONArrayBuilder childrenArrayBuilder(
-                            expandedNodesArrayBuilder.subarrayStart());
-                    const vector<AreaStats>& children = branch[depth];
-                    for (unsigned int child = 0; child < children.size(); ++child) {
-                        BSONObjBuilder childBuilder(childrenArrayBuilder.subobjStart());
-                        children[child].appendTo(childBuilder);
-                        childBuilder.doneFast();
-                    }
-                }
-                expandedNodesArrayBuilder.doneFast();
-            }
-        }
-    };
-
-    /**
-     * Performs the btree analysis for a generic btree version. After inspect() is called on the
-     * tree root, statistics are available through stats().
-     *
-     * Template-based implementation in BtreeInspectorImpl.
-     */
-    class BtreeInspector : boost::noncopyable {
-    public:
-        virtual ~BtreeInspector() {}
-        virtual bool inspect(const DiskLoc& head) = 0;
-        virtual BtreeStats& stats() = 0;
-    };
-
-    // See BtreeInspector.
-    template <class Version>
-    class BtreeInspectorImpl : public BtreeInspector {
-    public:
-        typedef typename mongo::BucketBasics<Version> BucketBasics;
-        typedef typename mongo::BucketBasics<Version>::_KeyNode _KeyNode;
-        typedef typename mongo::BucketBasics<Version>::KeyNode KeyNode;
-        typedef typename mongo::BucketBasics<Version>::Key Key;
-
-        BtreeInspectorImpl(vector<int> expandNodes) : _expandNodes(expandNodes) {
-        }
-
-        virtual bool inspect(const DiskLoc& head)  {
-            _stats.bucketBodyBytes = BucketBasics::bodySize();
-            vector<int> expandedAncestors;
-            return this->inspectBucket(head, 0, 0, true, expandedAncestors);
-        }
-
-        virtual BtreeStats& stats() {
-            return _stats;
-        }
-
-    private:
-        /**
-         * Recursively inspect btree buckets.
-         * @param dl DiskLoc for the current bucket
-         * @param depth depth for the current bucket (root is 0)
-         * @param childNum so that the current bucket is the childNum-th child of its parent
-         *                 (the right child is numbered as the last left child + 1)
-         * @param parentIsExpanded bucket expansion was requested for the parent bucket so the
-         *                         statistics and information for this bucket will appear in the
-         *                         subtree
-         * @param expandedAncestors if the d-th element is k, the k-th child of an expanded parent
-         *                          at depth d is expanded
-         *                          [0, 4, 1] means that root is expanded, its 4th child is expanded
-         *                          and, in turn, the first child of the 4th child of the root is
-         *                          expanded
-         * @return true on success, false otherwise
-         */
-        bool inspectBucket(const DiskLoc& dl, unsigned int depth, int childNum,
-                           bool parentIsExpanded, vector<int> expandedAncestors) {
-
-            if (dl.isNull()) return true;
-            killCurrentOp.checkForInterrupt();
-
-            const BtreeBucket<Version>* bucket = dl.btree<Version>();
-            int usedKeyCount = 0; // number of used keys in this bucket
-
-            int keyCount = bucket->getN();
-            int childrenCount = keyCount + 1; // maximum number of children of this bucket
-                                              // including the right child
-
-            if (depth > _stats.depth) _stats.depth = depth;
-
-            bool curNodeIsExpanded = false;
-            if (parentIsExpanded) {
-                // if the parent node is expanded, statistics and info will be outputted for this
-                // bucket as well
-                expandedAncestors.push_back(childNum);
-
-                    // if the expansion of this node was requested
-                if (depth < _expandNodes.size() && _expandNodes[depth] == childNum) {
-                    verify(_stats.branch.size() == depth + 1);
-                    _stats.newBranchLevel(childrenCount);
-                    curNodeIsExpanded = true;
-                }
-            }
-
-            const _KeyNode* firstKeyNode = NULL;
-            const _KeyNode* lastKeyNode = NULL;
-            for (int i = 0; i < keyCount; i++ ) {
-                const _KeyNode& kn = bucket->k(i);
-
-                if (kn.isUsed()) {
-                    ++usedKeyCount;
-                    if (i == 0) {
-                        firstKeyNode = &kn;
-                    }
-                    lastKeyNode = &kn;
-
-                    this->inspectBucket(kn.prevChildBucket, depth + 1, i, curNodeIsExpanded,
-                                        expandedAncestors);
-                }
-            }
-            this->inspectBucket(bucket->getNextChild(), depth + 1, keyCount, curNodeIsExpanded,
-                                expandedAncestors);
-
-            killCurrentOp.checkForInterrupt();
-
-            if (parentIsExpanded) {
-                // stats for the children of this bucket have been added in the recursive calls,
-                // avoid including the current bucket in the stats for its subtree
-                expandedAncestors.pop_back();
-            }
-
-
-            // add the stats for the current bucket to the aggregates for all its ancestors and
-            // the entire tree
-            for (unsigned int d = 0; d < expandedAncestors.size(); ++d) {
-                AreaStats& nodeStats = _stats.nodeAt(d, expandedAncestors[d]);
-                nodeStats.addStats(keyCount, usedKeyCount, bucket, sizeof(_KeyNode));
-            }
-            _stats.wholeTree.addStats(keyCount, usedKeyCount, bucket, sizeof(_KeyNode));
-
-            if (parentIsExpanded) {
-                NodeInfo nodeInfo;
-                if (firstKeyNode != NULL) {
-                    nodeInfo.firstKey = KeyNode(*bucket, *firstKeyNode).key.toBson();
-                }
-                if (lastKeyNode != NULL) {
-                    nodeInfo.lastKey = KeyNode(*bucket, *lastKeyNode).key.toBson();
-                }
-
-                nodeInfo.childNum = childNum;
-                nodeInfo.depth = depth;
-                nodeInfo.diskLoc = dl.toBSONObj();
-                nodeInfo.keyCount = keyCount;
-                nodeInfo.usedKeyCount = bucket->getN();
-                nodeInfo.fillRatio =
-                    (1.0 - static_cast<double>(bucket->getEmptySize()) / BucketBasics::bodySize());
-
-                _stats.nodeAt(depth, childNum).nodeInfo = nodeInfo;
-            }
-
-            // add the stats for this bucket to the aggregate for a certain depth
-            while (_stats.perLevel.size() < depth + 1)
-                _stats.perLevel.push_back(AreaStats());
-            verify(_stats.perLevel.size() > depth);
-            AreaStats& level = _stats.perLevel[depth];
-            level.addStats(keyCount, usedKeyCount, bucket, sizeof(_KeyNode));
-
-            return true;
-        } 
-
-        vector<int> _expandNodes;
-        BtreeStats _stats;
-    };
-
-    typedef BtreeInspectorImpl<V0> BtreeInspectorV0;
-    typedef BtreeInspectorImpl<V1> BtreeInspectorV1;
-
-    /**
-     * Run analysis with the provided parameters. See IndexStatsCmd for in-depth expanation of
-     * output.
-     *
-     * @return true on success, false otherwise
-     */
-    bool runInternal(const Collection* collection, IndexStatsParams params, string& errmsg,
-                     BSONObjBuilder& result) {
-
-        const IndexCatalog* indexCatalog = collection->getIndexCatalog();
-
-        IndexDescriptor* descriptor = indexCatalog->findIndexByName( params.indexName );
-
-        if (descriptor == NULL) {
-            errmsg = "the requested index does not exist";
-            return false;
-        }
-
-        result << "index" << descriptor->indexName()
-               << "version" << descriptor->version()
-               << "isIdIndex" << descriptor->isIdIndex()
-               << "keyPattern" << descriptor->keyPattern()
-               << "storageNs" << descriptor->indexNamespace();
-
-        scoped_ptr<BtreeInspector> inspector(NULL);
-        switch (descriptor->version()) {
-          case 1: inspector.reset(new BtreeInspectorV1(params.expandNodes)); break;
-          case 0: inspector.reset(new BtreeInspectorV0(params.expandNodes)); break;
-          default:
-            errmsg = str::stream() << "index version " << descriptor->version() << " is "
-                                   << "not supported";
-            return false;
-        }
-
-        inspector->inspect( indexCatalog->getEntry( descriptor )->head() );
-
-        inspector->stats().appendTo(result);
-
-        return true;
-    }
-
-    // Command
-
-    /**
-     * This command provides detailed and aggregate information and statistics for a btree. 
-     * Stats are aggregated for the entire tree, per-depth and, if requested through the expandNodes
-     * option, per-subtree.
-     * The entire btree is walked depth-first on every call. This command takes a read lock and may
-     * be slow for large indexes if the underlying extents arent't already in physical memory.
-     *
-     * The output has the form:
-     *     { index: <index name>,
-     *       version: <index version (0 or 1),
-     *       isIdKey: <true if this is the default _id index>,
-     *       keyPattern: <bson object describing the key pattern>,
-     *       storageNs: <namespace of the index's underlying storage>,
-     *       bucketBodyBytes: <bytes available for keynodes and bson objects in the bucket's body>,
-     *       depth: <index depth (root excluded)>
-     *       overall: { (statistics for the entire tree)
-     *           numBuckets: <number of buckets (samples)>
-     *           keyCount: { (stats about the number of keys in a bucket)
-     *               count: <number of samples>,
-     *               mean: <mean>
-     *    (optional) stddev: <standard deviation>
-     *    (optional) min: <minimum value (number of keys for the bucket that has the least)>
-     *    (optional) max: <maximum value (number of keys for the bucket that has the most)>
-     *    (optional) quantiles: {
-     *                   0.01: <1st percentile>, 0.02: ..., 0.09: ..., 0.25: <1st quartile>,
-     *                   0.5: <median>, 0.75: <3rd quartile>, 0.91: ..., 0.98: ..., 0.99: ...
-     *               }
-     *    (optional fields are only present if there are enough samples to compute sensible
-     *     estimates)
-     *           }
-     *           usedKeyCount: <stats about the number of used keys in a bucket>
-     *               (same structure as keyCount)
-     *           bsonRatio: <stats about how much of the bucket body is occupied by bson objects>
-     *               (same structure as keyCount)
-     *           keyNodeRatio: <stats about how much of the bucket body is occupied by KeyNodes>
-     *               (same structure as keyCount)
-     *           fillRatio: <stats about how full is the bucket body (bson objects + KeyNodes)>
-     *               (same structure as keyCount)
-     *       },
-     *       perLevel: [ (statistics aggregated per depth)
-     *           (one element with the same structure as 'overall' for each btree level,
-     *            the first refers to the root)
-     *       ]
-     *     }
-     *
-     * If 'expandNodes: [array]' was specified in the parameters, an additional field named
-     * 'expandedNodes' is included in the output. It contains two nested arrays, such that the
-     * n-th element of the outer array contains stats for nodes at depth n (root is included) and
-     * the i-th element (0-based) of the inner array at depth n contains stats for the subtree
-     * rooted at the i-th child of the expanend node at depth (n - 1).
-     * Each element of the inner array has the same structure as 'overall' in the description above:
-     * it includes the aggregate stats for all the nodes in the subtree excluding the current
-     * bucket.
-     * It also contains an additional field 'nodeInfo' representing information for the current
-     * node:
-     *     { childNum: <i so that this is the (i + 1)-th child of the parent node>
-     *       keyCount: <number of keys in this bucket>
-     *       usedKeyCount: <number of non-empty KeyNodes>
-     *       diskLoc: { (bson representation of the disk location for this bucket)
-     *           file: <num>
-     *           offset: <bytes>
-     *       }
-     *       depth: <depth of this bucket, root is at depth 0>
-     *       fillRatio: <a value between 0 and 1 representing how full this bucket is>
-     *       firstKey: <bson object containing the value for the first key>
-     *       lastKey: <bson object containing the value for the last key>
-     *     }
-     *
-     */
-    class IndexStatsCmd : public Command {
-    public:
-        IndexStatsCmd() : Command("indexStats") {}
-
-        virtual bool slaveOk() const {
-            return true;
-        }
-
-        virtual void help(stringstream& h) const {
-            h << "EXPERIMENTAL (UNSUPPORTED). "
-              << "Provides detailed and aggregate information and statistics for a btree. "
-              << "The entire btree is walked on every call. This command takes a read lock, "
-              << "requires the entire btree storage to be paged-in and will be slow on large "
-              << "indexes. Requires an index name in {index: '_name'}. Accepts and optional array "
-              << "of the nodes to be expanded, {expandNodes: [...]}. "
-              << "For example, {indexStats: 'collection', index: '_id', expandNodes: [0, 4]} "
-              << "aggregates statistics for the _id index for 'collection' and expands root "
-              << "and the fifth child of root.";
-        }
-
-        virtual bool isWriteCommandForConfigServer() const { return false; }
-
-        virtual void addRequiredPrivileges(const std::string& dbname,
-                                           const BSONObj& cmdObj,
-                                           std::vector<Privilege>* out) {
-            ActionSet actions;
-            actions.addAction(ActionType::indexStats);
-            out->push_back(Privilege(parseResourcePattern(dbname, cmdObj), actions));
-        }
-
-        bool run(OperationContext* txn, const string& dbname, BSONObj& cmdObj, int, string& errmsg,
-                 BSONObjBuilder& result, bool fromRepl) {
-
-            NamespaceString nss( dbname, cmdObj.firstElement().valuestrsafe() );
-            if (!serverGlobalParams.quiet) {
-                MONGO_TLOG(0) << "CMD: indexStats " << nss;
-            }
-
-            Client::ReadContext ctx(nss.ns());
-
-            const Collection* collection = ctx.ctx().db()->getCollection( nss.ns() );
-            if (!collection) {
-                errmsg = "ns not found";
-                return false;
-            }
-
-            IndexStatsParams params;
-
-            // { index: _index_name }
-            BSONElement indexName = cmdObj["index"];
-            if (!indexName.ok() || indexName.type() != String) {
-                errmsg = "an index name is required, use {index: \"indexname\"}";
-                return false;
-            }
-            params.indexName = indexName.String();
-
-            BSONElement expandNodes = cmdObj["expandNodes"];
-            if (expandNodes.ok()) {
-                if (expandNodes.type() != mongo::Array) {
-                    errmsg = "expandNodes must be an array of numbers";
-                    return false;
-                }
-                vector<BSONElement> arr = expandNodes.Array();
-                for (vector<BSONElement>::const_iterator it = arr.begin(); it != arr.end(); ++it) {
-                    if (!it->isNumber()) {
-                        errmsg = "expandNodes must be an array of numbers";
-                        return false;
-                    }
-                    params.expandNodes.push_back(int(it->Number()));
-                }
-            }
-
-            BSONObjBuilder resultBuilder;
-            if (!runInternal(collection, params, errmsg, resultBuilder))
-                return false;
-            result.appendElements(resultBuilder.obj());
-            return true;
-        }
-
-    };
-
-    MONGO_INITIALIZER(IndexStatsCmd)(InitializerContext* context) {
-        if (serverGlobalParams.experimental.indexStatsCmdEnabled) {
-            // Leaked intentionally: a Command registers itself when constructed.
-            new IndexStatsCmd();
-        }
-        return Status::OK();
-    }
-
-} // namespace mongo
-
-#endif
diff --git a/src/mongo/db/commands/storage_details.cpp b/src/mongo/db/commands/storage_details.cpp
deleted file mode 100644
index cd24cb156d..0000000000
--- a/src/mongo/db/commands/storage_details.cpp
+++ /dev/null
@@ -1,884 +0,0 @@
-/*    Copyright 2012 10gen Inc.
- *
- *    This program is free software: you can redistribute it and/or  modify
- *    it under the terms of the GNU Affero General Public License, version 3,
- *    as published by the Free Software Foundation.
- *
- *    This program is distributed in the hope that it will be useful,
- *    but WITHOUT ANY WARRANTY; without even the implied warranty of
- *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- *    GNU Affero General Public License for more details.
- *
- *    You should have received a copy of the GNU Affero General Public License
- *    along with this program.  If not, see <http://www.gnu.org/licenses/>.
- *
- *    As a special exception, the copyright holders give permission to link the
- *    code of portions of this program with the OpenSSL library under certain
- *    conditions as described in each individual source file and distribute
- *    linked combinations including the program with the OpenSSL library. You
- *    must comply with the GNU Affero General Public License in all respects
- *    for all of the code used other than as permitted herein. If you modify
- *    file(s) with this exception, you may extend this exception to your
- *    version of the file(s), but you are not obligated to do so. If you do not
- *    wish to do so, delete this exception statement from your version. If you
- *    delete this exception statement from all source files in the program,
- *    then also delete it in the license file.
- */
-
-#if 0 // disable SERVER-13555
-#include "mongo/pch.h"
-
-#include <ctime>
-#include <string>
-
-#include "mongo/base/init.h"
-#include "mongo/db/auth/action_set.h"
-#include "mongo/db/auth/action_type.h"
-#include "mongo/db/auth/privilege.h"
-#include "mongo/db/commands.h"
-#include "mongo/db/db.h"
-#include "mongo/db/jsobj.h"
-#include "mongo/db/kill_current_op.h"
-#include "mongo/db/catalog/collection.h"
-#include "mongo/util/processinfo.h"
-#include "mongo/util/mongoutils/str.h"
-
-namespace mongo {
-
-namespace {
-
-    // Helper classes and functions
-
-    /**
-     * Available subcommands.
-     */
-    enum SubCommand {
-        SUBCMD_DISK_STORAGE,
-        SUBCMD_PAGES_IN_RAM
-    };
-
-    /**
-     * Simple struct to store various operation parameters to be passed around during analysis.
-     */
-    struct AnalyzeParams {
-        // startOfs and endOfs are extent-relative
-        int startOfs;
-        int endOfs;
-        int length;
-        int numberOfSlices;
-        int granularity;
-        int lastSliceLength;
-        string characteristicField;
-        bool processDeletedRecords;
-        bool showRecords;
-        time_t startTime;
-
-        AnalyzeParams() : startOfs(0), endOfs(INT_MAX), length(INT_MAX), numberOfSlices(0),
-                          granularity(0), lastSliceLength(0), characteristicField("_id"),
-                          processDeletedRecords(true), showRecords(false), startTime(time(NULL)) {
-        }
-    };
-
-    /**
-     * Aggregated information per slice / extent.
-     */
-    struct DiskStorageData {
-        double numEntries;
-        long long bsonBytes;
-        long long recBytes;
-        long long onDiskBytes;
-        double characteristicSum;
-        double characteristicCount;
-        double outOfOrderRecs;
-        double freeRecords[mongo::Buckets];
-
-        DiskStorageData(long long diskBytes) : numEntries(0), bsonBytes(0), recBytes(0),
-                                               onDiskBytes(diskBytes), characteristicSum(0),
-                                               characteristicCount(0), outOfOrderRecs(0),
-                                               freeRecords() /* initialize array with zeroes */ {
-        }
-
-        const DiskStorageData& operator += (const DiskStorageData& rhs) {
-            this->numEntries += rhs.numEntries;
-            this->recBytes += rhs.recBytes;
-            this->bsonBytes += rhs.bsonBytes;
-            this->onDiskBytes += rhs.onDiskBytes;
-            this->characteristicSum += rhs.characteristicSum;
-            this->characteristicCount += rhs.characteristicCount;
-            this->outOfOrderRecs += rhs.outOfOrderRecs;
-            for (int i = 0; i < mongo::Buckets; i++) {
-                this->freeRecords[i] += rhs.freeRecords[i];
-            }
-            return *this;
-        }
-
-        void appendToBSONObjBuilder(BSONObjBuilder& b, bool includeFreeRecords) const {
-            b.append("numEntries", numEntries);
-            b.append("bsonBytes", bsonBytes);
-            b.append("recBytes", recBytes);
-            b.append("onDiskBytes", onDiskBytes);
-            b.append("outOfOrderRecs", outOfOrderRecs);
-            if (characteristicCount > 0) {
-                b.append("characteristicCount", characteristicCount);
-                b.append("characteristicAvg", characteristicSum / characteristicCount);
-            }
-            if (includeFreeRecords) {
-                BSONArrayBuilder freeRecsPerBucketArrBuilder(b.subarrayStart("freeRecsPerBucket"));
-                for (int i = 0; i < mongo::Buckets; i++) {
-                    freeRecsPerBucketArrBuilder.append(freeRecords[i]);
-                }
-                freeRecsPerBucketArrBuilder.doneFast();
-            }
-        }
-    };
-
-    /**
-     * Helper to calculate which slices the current record overlaps and how much of the record
-     * is in each of them.
-     * E.g.
-     *                 3.5M      4M     4.5M      5M      5.5M       6M
-     *     slices ->    |   12   |   13   |   14   |   15   |   16   |
-     *     record ->         [-------- 1.35M --------]
-     *
-     * results in something like:
-     *     firstSliceNum = 12
-     *     lastSliceNum = 15
-     *     sizeInFirstSlice = 0.25M
-     *     sizeInLastSlice = 0.10M
-     *     sizeInMiddleSlice = 0.5M (== size of slice)
-     *     inFirstSliceRatio = 0.25M / 1.35M = 0.185...
-     *     inLastSliceRatio = 0.10M / 1.35M = 0.074...
-     *     inMiddleSliceRatio = 0.5M / 1.35M = 0.37...
-     *
-     * The quasi-iterator SliceIterator is available to easily iterate over the slices spanned
-     * by the record and to obtain how much of the records belongs to each.
-     *
-     *    for (RecPos::SliceIterator it = pos.iterateSlices(); !it.end(); ++it) {
-     *        RecPos::SliceInfo res = *it;
-     *        // res contains the current slice number, the number of bytes belonging to the current
-     *        // slice, and the ratio with the full size of the record
-     *    }
-     *
-     */
-    struct RecPos {
-        bool outOfRange;
-        int firstSliceNum;
-        int lastSliceNum;
-        int sizeInFirstSlice;
-        int sizeInLastSlice;
-        int sizeInMiddleSlice;
-        double inFirstSliceRatio;
-        double inLastSliceRatio;
-        double inMiddleSliceRatio;
-        int numberOfSlices;
-
-        /**
-         * Calculate position of record among slices.
-         * @param recOfs record offset as reported by DiskLoc
-         * @param recLen record on-disk size with headers included
-         * @param extentOfs extent offset as reported by DiskLoc
-         * @param params operation parameters (see AnalyzeParams for details)
-         */
-        static RecPos from(int recOfs, int recLen, int extentOfs, const AnalyzeParams& params) {
-            RecPos res = {};
-            res.numberOfSlices = params.numberOfSlices;
-            // startsAt and endsAt are extent-relative
-            int startsAt = recOfs - extentOfs;
-            int endsAt = startsAt + recLen;
-            if (endsAt < params.startOfs || startsAt >= params.endOfs) {
-                res.outOfRange = true;
-                return res;
-            }
-            else {
-                res.outOfRange = false;
-            }
-            res.firstSliceNum = (startsAt - params.startOfs) / params.granularity;
-            res.lastSliceNum = (endsAt - params.startOfs) / params.granularity;
-
-            // extent-relative
-            int endOfFirstSlice = (res.firstSliceNum + 1) * params.granularity + params.startOfs;
-            res.sizeInFirstSlice = min(endOfFirstSlice - startsAt, recLen);
-            res.sizeInMiddleSlice = params.granularity;
-            res.sizeInLastSlice = recLen - res.sizeInFirstSlice -
-                                  params.granularity * (res.lastSliceNum - res.firstSliceNum
-                                                        - 1);
-            if (res.sizeInLastSlice < 0) {
-                res.sizeInLastSlice = 0;
-            }
-            res.inFirstSliceRatio = static_cast<double>(res.sizeInFirstSlice) / recLen;
-            res.inMiddleSliceRatio = static_cast<double>(res.sizeInMiddleSlice) / recLen;
-            res.inLastSliceRatio = static_cast<double>(res.sizeInLastSlice) / recLen;
-            return res;
-        }
-
-        // See RecPos class description
-        struct SliceInfo {
-            int sliceNum;
-            int sizeHere;
-            double ratioHere;
-        };
-
-        /**
-         * Iterates over slices spanned by the record.
-         */
-        class SliceIterator {
-        public:
-            SliceIterator(RecPos& pos) : _pos(pos), _valid(false) {
-                _curSlice.sliceNum = pos.firstSliceNum >= 0 ? _pos.firstSliceNum : 0;
-            }
-
-            bool end() const {
-                return _pos.outOfRange 
-                    || _curSlice.sliceNum >= _pos.numberOfSlices
-                    || _curSlice.sliceNum > _pos.lastSliceNum;
-            }
-
-            SliceInfo* operator->() {
-                return get();
-            }
-
-            SliceInfo& operator*() {
-                return *(get());
-            }
-
-            // preincrement
-            SliceIterator& operator++() {
-                _curSlice.sliceNum++;
-                _valid = false;
-                return *this;
-            }
-
-        private:
-            SliceInfo* get() {
-                verify(!end());
-                if (!_valid) {
-                    if (_curSlice.sliceNum == _pos.firstSliceNum) {
-                        _curSlice.sizeHere = _pos.sizeInFirstSlice;
-                        _curSlice.ratioHere = _pos.inFirstSliceRatio;
-                    }
-                    else if (_curSlice.sliceNum == _pos.lastSliceNum) {
-                        _curSlice.sizeHere = _pos.sizeInLastSlice;
-                        _curSlice.ratioHere = _pos.inLastSliceRatio;
-                    }
-                    else {
-                        DEV verify(_pos.firstSliceNum < _curSlice.sliceNum &&
-                                   _curSlice.sliceNum < _pos.lastSliceNum);
-                        _curSlice.sizeHere = _pos.sizeInMiddleSlice;
-                        _curSlice.ratioHere = _pos.inMiddleSliceRatio;
-                    }
-                    verify(_curSlice.sizeHere >= 0 && _curSlice.ratioHere >= 0);
-                    _valid = true;
-                }
-                return &_curSlice;
-            }
-
-            RecPos& _pos;
-            SliceInfo _curSlice;
-
-            // if _valid, data in _curSlice refers to the current slice, otherwise it needs
-            // to be computed
-            bool _valid;
-        };
-
-        SliceIterator iterateSlices() {
-            return SliceIterator(*this);
-        }
-    };
-
-    inline unsigned ceilingDiv(unsigned dividend, unsigned divisor) {
-        return (dividend + divisor - 1) / divisor;
-    }
-
-    // Command
-
-    /**
-     * This command provides detailed and aggreate information regarding record and deleted record
-     * layout in storage files and in memory.
-     */
-    class StorageDetailsCmd : public Command {
-    public:
-        StorageDetailsCmd() : Command( "storageDetails" ) {}
-
-        virtual bool slaveOk() const {
-            return true;
-        }
-
-        virtual void help(stringstream& h) const {
-            h << "EXPERIMENTAL (UNSUPPORTED). "
-              << "Provides detailed and aggregate information regarding record and deleted record "
-              << "layout in storage files ({analyze: 'diskStorage'}) and percentage of pages "
-              << "currently in RAM ({analyze: 'pagesInRAM'}). Slow if run on large collections. "
-              << "Select the desired subcommand with {analyze: 'diskStorage' | 'pagesInRAM'}; "
-              << "specify {extent: num_} and, optionally, {range: [start, end]} to restrict "
-              << "processing to a single extent (start and end are offsets from the beginning of "
-              << "the extent. {granularity: bytes} or {numberOfSlices: num_} enable aggregation of "
-              << "statistic per-slice: the extent(s) will either be subdivided in about "
-              << "'numberOfSlices' slices or in slices of size 'granularity'. "
-              << "{characteristicField: dotted_path} enables collection of a field to make "
-              << "it possible to identify which kind of record belong to each slice/extent. "
-              << "{showRecords: true} enables a dump of all records and deleted records "
-              << "encountered. Example: "
-              << "{storageDetails: 'collectionName', analyze: 'diskStorage', granularity: 1 << 20}";
-        }
-
-        virtual bool isWriteCommandForConfigServer() const { return false; }
-
-        virtual void addRequiredPrivileges(const std::string& dbname,
-                                           const BSONObj& cmdObj,
-                                           std::vector<Privilege>* out) {
-            ActionSet actions;
-            actions.addAction(ActionType::storageDetails);
-            out->push_back(Privilege(parseResourcePattern(dbname, cmdObj), actions));
-        }
-
-    private:
-        /**
-         * Entry point, parses command parameters and invokes runInternal.
-         */
-        bool run(OperationContext* txn, const string& dbname , BSONObj& cmdObj, int, string& errmsg,
-                 BSONObjBuilder& result, bool fromRepl);
-
-    };
-
-    MONGO_INITIALIZER(StorageDetailsCmd)(InitializerContext* context) {
-        if (serverGlobalParams.experimental.storageDetailsCmdEnabled) {
-            // Leaked intentionally: a Command registers itself when constructed.
-            new StorageDetailsCmd();
-        }
-        return Status::OK();
-    }
-
-    /**
-     * Extracts the characteristic field from the document, if present and of the type ObjectId,
-     * Date or numeric.
-     * @param obj the document
-     * @param value out: characteristic field value, only valid if true is returned
-     * @return true if field was correctly extracted, false otherwise (missing or of wrong type)
-     */
-    bool extractCharacteristicFieldValue(BSONObj& obj, const AnalyzeParams& params, double& value) {
-        BSONElement elem = obj.getFieldDotted(params.characteristicField);
-        if (elem.eoo()) {
-            return false;
-        }
-        bool hasValue = false;
-        if (elem.type() == jstOID) {
-            value = static_cast<double>(elem.OID().asTimeT());
-            hasValue = true;
-        }
-        else if (elem.isNumber()) {
-            value = elem.numberDouble();
-            hasValue = true;
-        }
-        else if (elem.type() == mongo::Date) {
-            value = static_cast<double>(elem.date().toTimeT());
-            hasValue = true;
-        }
-        return hasValue;
-    }
-
-    /**
-     * @return the requested extent if it exists, otherwise NULL
-     */
-    const Extent* getNthExtent(Database* db, int extentNum, const NamespaceDetails* nsd) {
-        int curExtent = 0;
-        for (Extent* ex = db->getExtentManager().getExtent(nsd->firstExtent());
-             ex != NULL;
-             ex = db->getExtentManager().getNextExtent(ex)) {
-
-            if (curExtent == extentNum) return ex;
-            curExtent++;
-        }
-        return NULL;
-    }
-
-    /**
-     * analyzeDiskStorage helper which processes a single record.
-     */
-    void processDeletedRecord(const DiskLoc& dl, const DeletedRecord* dr, const Extent* ex,
-                              const AnalyzeParams& params, int bucketNum,
-                              vector<DiskStorageData>& sliceData,
-                              BSONArrayBuilder* deletedRecordsArrayBuilder) {
-
-        killCurrentOp.checkForInterrupt();
-
-        int extentOfs = ex->myLoc.getOfs();
-
-        if (! (dl.a() == ex->myLoc.a() &&
-               dl.getOfs() + dr->lengthWithHeaders() > extentOfs &&
-               dl.getOfs() < extentOfs + ex->length) ) {
-
-            return;
-        }
-
-        RecPos pos = RecPos::from(dl.getOfs(), dr->lengthWithHeaders(), extentOfs, params);
-        bool spansRequestedArea = false;
-        for (RecPos::SliceIterator it = pos.iterateSlices(); !it.end(); ++it) {
-
-            DiskStorageData& slice = sliceData[it->sliceNum];
-            slice.freeRecords[bucketNum] += it->ratioHere;
-            spansRequestedArea = true;
-        }
-
-        if (deletedRecordsArrayBuilder != NULL && spansRequestedArea) {
-            BSONObjBuilder(deletedRecordsArrayBuilder->subobjStart())
-                .append("ofs", dl.getOfs() - extentOfs)
-                .append("recBytes", dr->lengthWithHeaders());
-        }
-    }
-
-    /**
-     * analyzeDiskStorage helper which processes a single record.
-     */
-    void processRecord(const DiskLoc& dl, const DiskLoc& prevDl, const Record* r, int extentOfs,
-                       const AnalyzeParams& params, vector<DiskStorageData>& sliceData,
-                       BSONArrayBuilder* recordsArrayBuilder) {
-        killCurrentOp.checkForInterrupt();
-
-        BSONObj obj = dl.obj();
-        int recBytes = r->lengthWithHeaders();
-        double characteristicFieldValue = 0;
-        bool hasCharacteristicField = extractCharacteristicFieldValue(obj, params,
-                                                                      characteristicFieldValue);
-        bool isLocatedBeforePrevious = dl.a() < prevDl.a();
-
-        RecPos pos = RecPos::from(dl.getOfs(), recBytes, extentOfs, params);
-        bool spansRequestedArea = false;
-        for (RecPos::SliceIterator it = pos.iterateSlices(); !it.end(); ++it) {
-            spansRequestedArea = true;
-            DiskStorageData& slice = sliceData[it->sliceNum];
-            slice.numEntries += it->ratioHere;
-            slice.recBytes += it->sizeHere;
-            slice.bsonBytes += static_cast<long long>(it->ratioHere * obj.objsize());
-            if (hasCharacteristicField) {
-                slice.characteristicCount += it->ratioHere;
-                slice.characteristicSum += it->ratioHere * characteristicFieldValue;
-            }
-            if (isLocatedBeforePrevious) {
-                slice.outOfOrderRecs += it->ratioHere;
-            }
-        }
-
-        if (recordsArrayBuilder != NULL && spansRequestedArea) {
-            DEV {
-                int startsAt = dl.getOfs() - extentOfs;
-                int endsAt = startsAt + recBytes;
-                verify((startsAt < params.startOfs && endsAt > params.startOfs) ||
-                       (startsAt < params.endOfs && endsAt >= params.endOfs) ||
-                       (startsAt >= params.startOfs && endsAt < params.endOfs));
-            }
-            BSONObjBuilder recordBuilder(recordsArrayBuilder->subobjStart());
-            recordBuilder.append("ofs", dl.getOfs() - extentOfs);
-            recordBuilder.append("recBytes", recBytes);
-            recordBuilder.append("bsonBytes", obj.objsize());
-            recordBuilder.append("_id", obj["_id"]);
-            if (hasCharacteristicField) {
-                recordBuilder.append("characteristic", characteristicFieldValue);
-            }
-            recordBuilder.doneFast();
-        }
-    }
-
-    // Top-level analysis functions
-
-    /**
-     * Provides aggregate and (if requested) detailed information regarding the layout of
-     * records and deleted records in the extent.
-     * The extent is split in params.numberOfSlices slices of params.granularity bytes each
-     * (except the last one which could be shorter).
-     * Iteration is performed over all records and deleted records in the specified (part of)
-     * extent and the output contains aggregate information for the entire record and per-slice.
-     * The typical output has the form:
-     *
-     *     { extentHeaderBytes: <size>,
-     *       recordHeaderBytes: <size>,
-     *       range: [startOfs, endOfs],     // extent-relative
-     *       numEntries: <number of records>,
-     *       bsonBytes: <total size of the bson objects>,
-     *       recBytes: <total size of the valid records>,
-     *       onDiskBytes: <length of the extent or range>,
-     * (opt) characteristicCount: <number of records containing the field used to tell them apart>
-     *       characteristicAvg: <average value of the characteristic field>
-     *       outOfOrderRecs: <number of records that follow - in the record linked list -
-     *                        a record that is located further in the extent>
-     * (opt) freeRecsPerBucket: [ ... ],
-     * The nth element in the freeRecsPerBucket array is the count of deleted records in the
-     * nth bucket of the deletedList.
-     * The characteristic field dotted path is specified in params.characteristicField.
-     * If its value is an OID or Date, the timestamp (as seconds since epoch) will be extracted;
-     * numeric values are converted to double and other bson types are ignored.
-     *
-     * The list of slices follows, with similar information aggregated per-slice:
-     *       slices: [
-     *           { numEntries: <number of records>,
-     *             ...
-     *             freeRecsPerBucket: [ ... ]
-     *           },
-     *           ...
-     *       ]
-     *     }
-     *
-     * If params.showRecords is set two additional fields are added to the outer document:
-     *       records: [
-     *           { ofs: <record offset from start of extent>,
-     *             recBytes: <record size>,
-     *             bsonBytes: <bson document size>,
-     *  (optional) characteristic: <value of the characteristic field>
-     *           }, 
-     *           ... (one element per record)
-     *       ],
-     *  (optional) deletedRecords: [
-     *           { ofs: <offset from start of extent>,
-     *             recBytes: <deleted record size>
-     *           },
-     *           ... (one element per deleted record)
-     *       ]
-     *
-     * @return true on success, false on failure (partial output may still be present)
-     */
-    bool analyzeDiskStorage(const Collection* collection, const Extent* ex,
-                            const AnalyzeParams& params, string& errmsg,
-                            BSONObjBuilder& result) {
-        bool isCapped = collection->isCapped();
-
-        result.append("extentHeaderBytes", Extent::HeaderSize());
-        result.append("recordHeaderBytes", Record::HeaderSize);
-        result.append("range", BSON_ARRAY(params.startOfs << params.endOfs));
-        result.append("isCapped", isCapped);
-
-        vector<DiskStorageData> sliceData(params.numberOfSlices,
-                                          DiskStorageData(params.granularity));
-        sliceData[params.numberOfSlices - 1].onDiskBytes = params.lastSliceLength;
-        Record* r;
-        int extentOfs = ex->myLoc.getOfs();
-
-        scoped_ptr<BSONArrayBuilder> recordsArrayBuilder;
-        if (params.showRecords) {
-            recordsArrayBuilder.reset(new BSONArrayBuilder(result.subarrayStart("records")));
-        }
-
-        Database* db = cc().database();
-        ExtentManager& extentManager = db->getExtentManager();
-
-        DiskLoc prevDl = ex->firstRecord;
-        for (DiskLoc dl = ex->firstRecord; !dl.isNull(); dl = extentManager.getNextRecordInExtent(dl)) {
-            r = collection->getRecordStore()->recordFor(dl);
-            processRecord(dl, prevDl, r, extentOfs, params, sliceData,
-                          recordsArrayBuilder.get());
-            prevDl = dl;
-        }
-        if (recordsArrayBuilder.get() != NULL) {
-            recordsArrayBuilder->doneFast();
-        }
-
-        bool processingDeletedRecords = !isCapped && params.processDeletedRecords;
-
-        scoped_ptr<BSONArrayBuilder> deletedRecordsArrayBuilder;
-        if (params.showRecords) {
-            deletedRecordsArrayBuilder.reset(
-                    new BSONArrayBuilder(result.subarrayStart("deletedRecords")));
-        }
-
-        if (processingDeletedRecords) {
-            for (int bucketNum = 0; bucketNum < mongo::Buckets; bucketNum++) {
-                DiskLoc dl = collection->details()->deletedListEntry(bucketNum);
-                while (!dl.isNull()) {
-                    const DeletedRecord* dr = collection->getRecordStore()->deletedRecordFor(dl);
-                    processDeletedRecord(dl, dr, ex, params, bucketNum, sliceData,
-                                         deletedRecordsArrayBuilder.get());
-                    dl = dr->nextDeleted();
-                }
-            }
-        }
-        if (deletedRecordsArrayBuilder.get() != NULL) {
-            deletedRecordsArrayBuilder->doneFast();
-        }
-
-        DiskStorageData extentData(0);
-        if (sliceData.size() > 1) {
-            BSONArrayBuilder sliceArrayBuilder (result.subarrayStart("slices"));
-            for (vector<DiskStorageData>::iterator it = sliceData.begin();
-                 it != sliceData.end(); ++it) {
-
-                killCurrentOp.checkForInterrupt();
-                extentData += *it;
-                BSONObjBuilder sliceBuilder;
-                it->appendToBSONObjBuilder(sliceBuilder, processingDeletedRecords);
-                sliceArrayBuilder.append(sliceBuilder.obj());
-            }
-            sliceArrayBuilder.doneFast();
-            extentData.appendToBSONObjBuilder(result, processingDeletedRecords);
-        } else {
-            sliceData[0].appendToBSONObjBuilder(result, processingDeletedRecords);
-        }
-        return true;
-    }
-
-    /**
-     * Outputs which percentage of pages are in memory for the entire extent and per-slice.
-     * Refer to analyzeDiskStorage for a description of what slices are.
-     *
-     * The output has the form:
-     *     { pageBytes: <system page size>,
-     *       onDiskBytes: <size of the extent>,
-     *       inMem: <ratio of pages in memory for the entire extent>,
-     * (opt) slices: [ ... ] (only present if either params.granularity or numberOfSlices is not
-     *                        zero and there exist more than one slice for this extent)
-     * (opt) sliceBytes: <size of each slice>
-     *     }
-     *
-     * The nth element in the slices array is the ratio of pages in memory for the nth slice.
-     *
-     * @return true on success, false on failure (partial output may still be present)
-     */
-    bool analyzePagesInRAM(const Extent* ex, const AnalyzeParams& params, string& errmsg,
-                           BSONObjBuilder& result) {
-
-        verify(sizeof(char) == 1);
-        int pageBytes = static_cast<int>(ProcessInfo::getPageSize());
-        result.append("pageBytes", pageBytes);
-        result.append("onDiskBytes", ex->length - Extent::HeaderSize());
-        char* startAddr = (char*) ex + params.startOfs;
-
-        int extentPages = ceilingDiv(params.endOfs - params.startOfs, pageBytes);
-        int extentInMemCount = 0; // number of pages in memory for the entire extent
-
-        scoped_ptr<BSONArrayBuilder> arr;
-        int sliceBytes = params.granularity;
-
-        if (params.numberOfSlices > 1) {
-            result.append("sliceBytes", sliceBytes);
-            arr.reset(new BSONArrayBuilder(result.subarrayStart("slices")));
-        }
-
-        for (int slice = 0; slice < params.numberOfSlices; slice++) {
-            if (slice == params.numberOfSlices - 1) {
-                sliceBytes = params.lastSliceLength;
-            }
-            int pagesInSlice = ceilingDiv(sliceBytes, pageBytes);
-
-            const char* firstPageAddr = startAddr + (slice * params.granularity);
-            vector<char> isInMem;
-            if (! ProcessInfo::pagesInMemory(firstPageAddr, pagesInSlice, &isInMem)) {
-                errmsg = "system call failed";
-                return false;
-            }
-
-            int inMemCount = 0;
-            for (int page = 0; page < pagesInSlice; page++) {
-                if (isInMem[page]) {
-                    inMemCount++;
-                    extentInMemCount++;
-                }
-            }
-
-            if (arr.get() != NULL) {
-                arr->append(static_cast<double>(inMemCount) / pagesInSlice);
-            }
-        }
-        if (arr.get() != NULL) {
-            arr->doneFast();
-        }
-
-        result.append("inMem", static_cast<double>(extentInMemCount) / extentPages);
-
-        return true;
-    }
-
-    /**
-     * Analyze a single extent.
-     * @param params analysis parameters, will be updated with computed number of slices or
-     *               granularity
-     */
-    bool analyzeExtent(const Collection* collection, const Extent* ex, SubCommand subCommand,
-                       AnalyzeParams& params, string& errmsg, BSONObjBuilder& outputBuilder) {
-
-        params.startOfs = max(0, params.startOfs);
-        params.endOfs = min(params.endOfs, ex->length);
-        params.length = params.endOfs - params.startOfs;
-        if (params.numberOfSlices != 0) {
-            params.granularity = (params.endOfs - params.startOfs + params.numberOfSlices - 1) /
-                                 params.numberOfSlices;
-        }
-        else if (params.granularity != 0) {
-            params.numberOfSlices = ceilingDiv(params.length, params.granularity);
-        }
-        else {
-            params.numberOfSlices = 1;
-            params.granularity = params.length;
-        }
-        params.lastSliceLength = params.length -
-                (params.granularity * (params.numberOfSlices - 1));
-        switch (subCommand) {
-            case SUBCMD_DISK_STORAGE:
-                return analyzeDiskStorage(collection, ex, params, errmsg, outputBuilder);
-            case SUBCMD_PAGES_IN_RAM:
-                return analyzePagesInRAM(ex, params, errmsg, outputBuilder);
-        }
-        verify(false && "unreachable");
-    }
-
-    /**
-     * @param ex requested extent; if NULL analyze entire namespace
-     */ 
-    bool runInternal(const Database* db, const Collection* collection, const Extent* ex,
-                     SubCommand subCommand, AnalyzeParams& globalParams,
-                     string& errmsg, BSONObjBuilder& result) {
-        const NamespaceDetails* nsd = collection->details();
-        const ExtentManager& em = db->getExtentManager();
-        BSONObjBuilder outputBuilder; // temporary builder to avoid output corruption in case of
-                                      // failure
-        bool success = false;
-        if (ex != NULL) {
-            success = analyzeExtent(collection, ex, subCommand, globalParams, errmsg, outputBuilder);
-        }
-        else {
-            const DiskLoc dl = nsd->firstExtent();
-            if (dl.isNull()) {
-                errmsg = "no extents in namespace";
-                return false;
-            }
-
-            long long storageSize = collection->storageSize(NULL, NULL);
-
-            if (globalParams.numberOfSlices != 0) {
-                globalParams.granularity = ceilingDiv(storageSize, globalParams.numberOfSlices);
-            }
-
-            BSONArrayBuilder extentsArrayBuilder(outputBuilder.subarrayStart("extents"));
-            for (Extent* curExtent = em.getExtent(dl);
-                 curExtent != NULL;
-                 curExtent = em.getNextExtent(curExtent)) {
-
-                AnalyzeParams extentParams(globalParams);
-                extentParams.numberOfSlices = 0; // use the specified or calculated granularity;
-                                                 // globalParams.numberOfSlices refers to the
-                                                 // total number of slices across all the
-                                                 // extents
-                BSONObjBuilder extentBuilder(extentsArrayBuilder.subobjStart());
-                success = analyzeExtent(collection, curExtent, subCommand, extentParams, errmsg,
-                                        extentBuilder);
-                extentBuilder.doneFast();
-            }
-            extentsArrayBuilder.doneFast();
-        }
-        if (!success) return false;
-        result.appendElements(outputBuilder.obj());
-        return true;
-    }
-
-    static const char* USE_ANALYZE_STR = "use {analyze: 'diskStorage' | 'pagesInRAM'}";
-
-    bool StorageDetailsCmd::run(OperationContext* txn, const string& dbname, BSONObj& cmdObj, int, string& errmsg,
-                                BSONObjBuilder& result, bool fromRepl) {
-
-        // { analyze: subcommand }
-        BSONElement analyzeElm = cmdObj["analyze"];
-        if (analyzeElm.eoo()) {
-            errmsg = str::stream() << "no subcommand specified, " << USE_ANALYZE_STR;
-            return false;
-        }
-
-        const char* subCommandStr = analyzeElm.valuestrsafe();
-        SubCommand subCommand;
-        if (str::equals(subCommandStr, "diskStorage")) {
-            subCommand = SUBCMD_DISK_STORAGE;
-        }
-        else if (str::equals(subCommandStr, "pagesInRAM")) {
-            subCommand = SUBCMD_PAGES_IN_RAM;
-        }
-        else {
-            errmsg = str::stream() << subCommandStr << " is not a valid subcommand, "
-                                                    << USE_ANALYZE_STR;
-            return false;
-        }
-
-        const string ns = dbname + "." + cmdObj.firstElement().valuestrsafe();
-
-        if (!serverGlobalParams.quiet) {
-            MONGO_TLOG(0) << "CMD: storageDetails " << ns << ", analyze " << subCommandStr << endl;
-        }
-
-        Client::ReadContext ctx(ns);
-
-        Database* db = cc().database();
-        const Collection* collection = db->getCollection( ns );
-        if (!collection) {
-            errmsg = "ns not found";
-            return false;
-        }
-        const NamespaceDetails* nsd = collection->details();
-
-        const Extent* extent = NULL;
-
-        // { extent: num }
-        BSONElement extentElm = cmdObj["extent"];
-        if (extentElm.ok()) {
-            if (!extentElm.isNumber()) {
-                errmsg = "extent number must be a number, e.g. {..., extent: 3, ...}";
-                return false;
-            }
-            int extentNum = extentElm.numberInt();
-            extent = getNthExtent(db, extentNum, nsd);
-            if (extent == NULL) {
-                errmsg = str::stream() << "extent " << extentNum << " does not exist";
-                return false;
-            }
-        }
-
-        AnalyzeParams params;
-
-        // { range: [from, to] }, extent-relative
-        BSONElement rangeElm = cmdObj["range"];
-        if (rangeElm.ok()) {
-            if (extent == NULL) {
-                errmsg = "a range is only allowed when a single extent is requested, "
-                         "use {..., extent: _num, range: [_a, _b], ...}";
-                return false;
-            }
-            if (!(rangeElm.type() == mongo::Array
-               && rangeElm["0"].isNumber()
-               && rangeElm["1"].isNumber()
-               && rangeElm["2"].eoo())) {
-                errmsg = "range must be an array with exactly two numeric elements";
-                return false;
-            }
-            params.startOfs = rangeElm["0"].numberInt();
-            params.endOfs = rangeElm["1"].numberInt();
-        }
-
-        // { granularity: bytes }
-        BSONElement granularityElm = cmdObj["granularity"];
-        if (granularityElm.ok() && !granularityElm.isNumber()) {
-            errmsg = "granularity must be a number";
-            return false;
-        }
-        params.granularity = granularityElm.numberInt();
-
-        // { numberOfSlices: num }
-        BSONElement numberOfSlicesElm = cmdObj["numberOfSlices"];
-        if (numberOfSlicesElm.ok() && !numberOfSlicesElm.isNumber()) {
-            errmsg = "numberOfSlices must be a number";
-            return false;
-        }
-        params.numberOfSlices = numberOfSlicesElm.numberInt();
-
-        BSONElement characteristicFieldElm = cmdObj["characteristicField"];
-        if (characteristicFieldElm.ok()) {
-            params.characteristicField = characteristicFieldElm.valuestrsafe();
-        }
-
-        BSONElement processDeletedRecordsElm = cmdObj["processDeletedRecords"];
-        if (processDeletedRecordsElm.ok()) {
-            params.processDeletedRecords = processDeletedRecordsElm.trueValue();
-        }
-
-        params.showRecords = cmdObj["showRecords"].trueValue();
-
-        return runInternal(db, collection, extent, subCommand, params, errmsg, result);
-    }
-
-}  // namespace
-
-}  // namespace mongo
-
-#endif

