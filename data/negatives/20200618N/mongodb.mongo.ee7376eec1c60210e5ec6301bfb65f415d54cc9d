commit ee7376eec1c60210e5ec6301bfb65f415d54cc9d
Author: Hugh Han <hughhan1@gmail.com>
Date:   Tue Jul 18 17:36:44 2017 -0400

    SERVER-30211 Create function inside CollectionShardingState to determine whether a chunk should be auto-split
    
    Currently, inside cluster_write.cpp's updateChunkWriteStatsAndSplitIfNeeded function, we check whether a chunk should be auto-split, depending on its current size and its desired chunk size. In this commit, we make this logic possible from within CollectionShardingState's onUpdateOp and onInsertOp functions.

diff --git a/src/mongo/db/s/collection_range_deleter_test.cpp b/src/mongo/db/s/collection_range_deleter_test.cpp
index 14528c4267..1763301fbb 100644
--- a/src/mongo/db/s/collection_range_deleter_test.cpp
+++ b/src/mongo/db/s/collection_range_deleter_test.cpp
@@ -44,6 +44,7 @@
 #include "mongo/db/s/collection_sharding_state.h"
 #include "mongo/db/s/sharding_state.h"
 #include "mongo/db/service_context_d_test_fixture.h"
+#include "mongo/s/balancer_configuration.h"
 #include "mongo/s/chunk_version.h"
 #include "mongo/s/client/shard_registry.h"
 #include "mongo/s/sharding_mongod_test_fixture.h"
@@ -78,6 +79,10 @@ protected:
         return _epoch;
     }
 
+    virtual std::unique_ptr<BalancerConfiguration> makeBalancerConfiguration() override {
+        return stdx::make_unique<BalancerConfiguration>();
+    }
+
 private:
     void setUp() override;
     void tearDown() override;
diff --git a/src/mongo/db/s/collection_sharding_state.cpp b/src/mongo/db/s/collection_sharding_state.cpp
index fa51f7b717..97248fc864 100644
--- a/src/mongo/db/s/collection_sharding_state.cpp
+++ b/src/mongo/db/s/collection_sharding_state.cpp
@@ -49,6 +49,7 @@
 #include "mongo/db/server_options.h"
 #include "mongo/db/server_parameters.h"
 #include "mongo/db/service_context.h"
+#include "mongo/s/balancer_configuration.h"
 #include "mongo/s/catalog/sharding_catalog_manager.h"
 #include "mongo/s/catalog/type_config_version.h"
 #include "mongo/s/catalog/type_shard.h"
@@ -282,16 +283,22 @@ bool CollectionShardingState::isDocumentInMigratingChunk(OperationContext* opCtx
 void CollectionShardingState::onInsertOp(OperationContext* opCtx, const BSONObj& insertedDoc) {
     dassert(opCtx->lockState()->isCollectionLockedForMode(_nss.ns(), MODE_IX));
 
-    if (serverGlobalParams.clusterRole == ClusterRole::ShardServer &&
-        _nss == NamespaceString::kServerConfigurationNamespace) {
-        if (auto idElem = insertedDoc["_id"]) {
-            if (idElem.str() == ShardIdentityType::IdName) {
-                auto shardIdentityDoc = uassertStatusOK(ShardIdentityType::fromBSON(insertedDoc));
-                uassertStatusOK(shardIdentityDoc.validate());
-                opCtx->recoveryUnit()->registerChange(
-                    new ShardIdentityLogOpHandler(opCtx, std::move(shardIdentityDoc)));
+    if (serverGlobalParams.clusterRole == ClusterRole::ShardServer) {
+        if (_nss == NamespaceString::kServerConfigurationNamespace) {
+            if (auto idElem = insertedDoc["_id"]) {
+                if (idElem.str() == ShardIdentityType::IdName) {
+                    auto shardIdentityDoc =
+                        uassertStatusOK(ShardIdentityType::fromBSON(insertedDoc));
+                    uassertStatusOK(shardIdentityDoc.validate());
+                    opCtx->recoveryUnit()->registerChange(
+                        new ShardIdentityLogOpHandler(opCtx, std::move(shardIdentityDoc)));
+                }
             }
         }
+
+        if (ShardingState::get(opCtx)->enabled()) {
+            _incrementChunkOnInsertOrUpdate(opCtx, insertedDoc, insertedDoc.objsize());
+        }
     }
 
     checkShardVersionOrThrow(opCtx);
@@ -307,9 +314,15 @@ void CollectionShardingState::onUpdateOp(OperationContext* opCtx,
                                          const BSONObj& updatedDoc) {
     dassert(opCtx->lockState()->isCollectionLockedForMode(_nss.ns(), MODE_IX));
 
-    if (serverGlobalParams.clusterRole == ClusterRole::ShardServer &&
-        _nss == NamespaceString::kShardConfigCollectionsCollectionName) {
-        _onConfigRefreshCompleteInvalidateCachedMetadataAndNotify(opCtx, query, update, updatedDoc);
+    if (serverGlobalParams.clusterRole == ClusterRole::ShardServer) {
+        if (_nss == NamespaceString::kShardConfigCollectionsCollectionName) {
+            _onConfigRefreshCompleteInvalidateCachedMetadataAndNotify(
+                opCtx, query, update, updatedDoc);
+        }
+
+        if (ShardingState::get(opCtx)->enabled()) {
+            _incrementChunkOnInsertOrUpdate(opCtx, updatedDoc, update.objsize());
+        }
     }
 
     checkShardVersionOrThrow(opCtx);
@@ -538,4 +551,58 @@ bool CollectionShardingState::_checkShardVersionOk(OperationContext* opCtx,
     MONGO_UNREACHABLE;
 }
 
+uint64_t CollectionShardingState::_incrementChunkOnInsertOrUpdate(OperationContext* opCtx,
+                                                                  const BSONObj& document,
+                                                                  long dataWritten) {
+
+    // Here, get the collection metadata and check if it exists. If it doesn't exist, then the
+    // collection is not sharded, and we can simply return -1.
+    ScopedCollectionMetadata metadata = getMetadata();
+    if (!metadata) {
+        return -1;
+    }
+
+    std::shared_ptr<ChunkManager> cm = metadata->getChunkManager();
+    const ShardKeyPattern& shardKeyPattern = cm->getShardKeyPattern();
+
+    // Each inserted/updated document should contain the shard key. The only instance in which a
+    // document could not contain a shard key is if the insert/update is performed through mongod
+    // explicitly, as opposed to first routed through mongos.
+    BSONObj shardKey = shardKeyPattern.extractShardKeyFromDoc(document);
+    if (shardKey.woCompare(BSONObj()) == 0) {
+        warning() << "inserting document " << document.toString() << " without shard key pattern "
+                  << shardKeyPattern << " into a sharded collection";
+        return -1;
+    }
+
+    // Use the shard key to locate the chunk into which the document was updated, and increment the
+    // number of bytes tracked for the chunk. Note that we can assume the simple collation, because
+    // shard keys do not support non-simple collations.
+    std::shared_ptr<Chunk> chunk = cm->findIntersectingChunkWithSimpleCollation(shardKey);
+    invariant(chunk);
+    chunk->addBytesWritten(dataWritten);
+
+    // If the chunk becomes too large, then we call the ChunkSplitter to schedule a split. Then, we
+    // reset the tracking for that chunk to 0.
+    if (_shouldSplitChunk(opCtx, shardKeyPattern, *chunk)) {
+        // TODO: call ChunkSplitter here
+        chunk->clearBytesWritten();
+    }
+    return chunk->getBytesWritten();
+}
+
+bool CollectionShardingState::_shouldSplitChunk(OperationContext* opCtx,
+                                                const ShardKeyPattern& shardKeyPattern,
+                                                const Chunk& chunk) {
+
+    const auto balancerConfig = Grid::get(opCtx)->getBalancerConfiguration();
+    invariant(balancerConfig);
+
+    const KeyPattern keyPattern = shardKeyPattern.getKeyPattern();
+    const bool minIsInf = (0 == keyPattern.globalMin().woCompare(chunk.getMin()));
+    const bool maxIsInf = (0 == keyPattern.globalMax().woCompare(chunk.getMax()));
+
+    return chunk.shouldSplit(balancerConfig->getMaxChunkSizeBytes(), minIsInf, maxIsInf);
+}
+
 }  // namespace mongo
diff --git a/src/mongo/db/s/collection_sharding_state.h b/src/mongo/db/s/collection_sharding_state.h
index 979a638599..0f98279922 100644
--- a/src/mongo/db/s/collection_sharding_state.h
+++ b/src/mongo/db/s/collection_sharding_state.h
@@ -43,6 +43,7 @@ namespace mongo {
 // How long to wait before starting cleanup of an emigrated chunk range.
 extern AtomicInt32 orphanCleanupDelaySecs;
 
+class BalancerConfiguration;
 class BSONObj;
 struct ChunkVersion;
 class CollectionMetadata;
@@ -285,6 +286,23 @@ private:
                               ChunkVersion* expectedShardVersion,
                               ChunkVersion* actualShardVersion);
 
+    /**
+     * If the collection is sharded, finds the chunk that contains the specified document, and
+     * increments the size tracked for that chunk by the specified amount of data written, in
+     * bytes. Returns the number of total bytes on that chunk, after the data is written.
+     */
+    uint64_t _incrementChunkOnInsertOrUpdate(OperationContext* opCtx,
+                                             const BSONObj& document,
+                                             long dataWritten);
+
+    /**
+     * Returns true if the total number of bytes on the specified chunk nears the max size of
+     * a shard.
+     */
+    bool _shouldSplitChunk(OperationContext* opCtx,
+                           const ShardKeyPattern& shardKeyPattern,
+                           const Chunk& chunk);
+
     // Namespace this state belongs to.
     const NamespaceString _nss;
 

