{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PatchClassificationRNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1gGy98fbogkWus-76M9OXwtxpbdh1mA5k",
      "authorship_tag": "ABX9TyPrfXAGsGYNpU0EHXV8BE7s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shuwang127/PatchClassificationRNN/blob/master/PatchClassificationRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fko-ZzuoPWWJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "0555d297-8d81-40b8-c46b-0931ce2a581d"
      },
      "source": [
        "'''\n",
        "    PatchClassificationRNN\n",
        "'''\n",
        "!pip install clang\n",
        "\n",
        "import os\n",
        "import gc\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import clang.cindex\n",
        "import clang.enumerations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torchdata\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# file path.\n",
        "rootPath = './drive/My Drive/Colab Notebooks/'\n",
        "dataPath = rootPath + '/data/'\n",
        "sDatPath = dataPath + '/security_patch/'\n",
        "pDatPath = dataPath + '/positives/'\n",
        "nDatPath = dataPath + '/negatives/'\n",
        "tempPath = rootPath + '/temp/'\n",
        "\n",
        "# hyper-parameters. (affect GPU memory)\n",
        "_DiffEmbedDim_  = 32\n",
        "_DiffMaxLen_    = 100\n",
        "_TRnnHidSiz_    = 16\n",
        "# hyper-parameters. (affect training speed)\n",
        "_TRnnBatchSz_   = 16\n",
        "_TRnnLearnRt_   = 0.0001\n",
        "# hyper-parameters. (unnecessary to modify)\n",
        "_DiffExtraDim_  = 2\n",
        "_TRnnHidLay_    = 1\n",
        "_TRnnMaxEpoch_  = 1000\n",
        "_TRnnPerEpoch_  = 1\n",
        "_TRnnJudEpoch_  = 1\n",
        "\n",
        "# control\n",
        "_DEBUG_ = 0 # 0 : release\n",
        "            # 1 : debug\n",
        "_LOCK_ = 0  # 0 : unlocked - create random split sets.\n",
        "            # 1 : locked   - use the saved split sets.\n",
        "_MODEL_ = 0 # 0 : unlocked - train a new model.\n",
        "            # 1 : locked   - load the saved model.\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: clang in /usr/local/lib/python3.6/dist-packages (6.0.0.2)\n",
            "[INFO] <ReadData> Load 38041 raw data from ./drive/My Drive/Colab Notebooks//temp//data.npy.\n",
            "[INFO] <GetDiffProps> Load 38041 diff property data from ./drive/My Drive/Colab Notebooks//temp//props.npy.\n",
            "[INFO] <GetDiffVocab> There are 654588 diff vocabulary tokens. (except '<pad>')\n",
            "[INFO] <GetDiffVocab> The max diff length is 2706522 tokens. (hyperparameter: _DiffMaxLen_ = 1000)\n",
            "[INFO] <GetDiffDict> Create dictionary for 654589 diff vocabulary tokens. (with '<pad>')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jbxy1piicVB7",
        "colab_type": "text"
      },
      "source": [
        "## ddd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-1GeS0GcIhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def demoTextRNN():\n",
        "    # load data.\n",
        "    if (not os.path.exists(tempPath + '/data.npy')): # | (not _DEBUG_)\n",
        "        dataLoaded = ReadData()\n",
        "    else:\n",
        "        dataLoaded = np.load(tempPath + '/data.npy', allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Load ' + str(len(dataLoaded)) + ' raw data from ' + tempPath + '/data.npy.')\n",
        "\n",
        "    # get the diff file properties.\n",
        "    if (not os.path.exists(tempPath + '/props.npy')):\n",
        "        diffProps = GetDiffProps(dataLoaded)\n",
        "    else:\n",
        "        diffProps = np.load(tempPath + '/props.npy', allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Load ' + str(len(diffProps)) + ' diff property data from ' + tempPath + '/props.npy.')\n",
        "\n",
        "    # get the diff token vocabulary.\n",
        "    diffVocab, diffMaxLen = GetDiffVocab(diffProps)\n",
        "    # get the max diff length.\n",
        "    diffMaxLen = _DiffMaxLen_ if (diffMaxLen > _DiffMaxLen_) else diffMaxLen\n",
        "    # get the diff token dictionary.\n",
        "    diffDict = GetDiffDict(diffVocab)\n",
        "    # get pre-trained weights for embedding layer.\n",
        "    diffPreWeights = GetDiffEmbed(diffDict, _DiffEmbedDim_)\n",
        "    # get the mapping for feature data and labels.\n",
        "    diffData, diffLabels = GetDiffMapping(diffProps, diffMaxLen, diffDict)\n",
        "    # change the tokentypes into one-hot vector.\n",
        "    diffData = UpdateTokenTypes(diffData)\n",
        "\n",
        "    # split data into rest/test dataset.\n",
        "    dataRest, labelRest, dataTest, labelTest = SplitData(diffData, diffLabels, 'test', rate=0.2)\n",
        "    # split data into train/valid dataset.\n",
        "    dataTrain, labelTrain, dataValid, labelValid = SplitData(dataRest, labelRest, 'valid', rate=0.2)\n",
        "    print('[INFO] <main> Get ' + str(len(dataTrain)) + ' Train data, ' + str(len(dataValid)) + ' VALID data, '\n",
        "          + str(len(dataTest)) + ' TEST data. (Total: ' + str(len(dataTrain)+len(dataValid)+len(dataTest)) + ')')\n",
        "\n",
        "    # TextRNNTrain\n",
        "    if (_MODEL_) & (os.path.exists(tempPath + '/model_TextRNN.pth')):\n",
        "        preWeights = torch.from_numpy(diffPreWeights)\n",
        "        model = TextRNN(preWeights, hiddenSize=_TRnnHidSiz_, hiddenLayers=_TRnnHidLay_)\n",
        "        model.load_state_dict(torch.load(tempPath + '/model_TextRNN.pth'))\n",
        "    else:\n",
        "        model = TextRNNTrain(dataTrain, labelTrain, dataValid, labelValid, preWeights=diffPreWeights,\n",
        "                             batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n",
        "\n",
        "    # TextRNNTest\n",
        "    predictions, accuracy = TextRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n",
        "    _, confusion = OutputEval(predictions, labelTest, 'TextRNN')\n",
        "\n",
        "    return\n",
        "\n",
        "def ReadData():\n",
        "    '''\n",
        "    Read data from the files.\n",
        "    :return: data - a set of commit message, diff code, and labels.\n",
        "    [[['', ...], [['', ...], ['', ...], ...], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def ReadCommitMsg(filename):\n",
        "        '''\n",
        "        Read commit message from a file.\n",
        "        :param filename: file name (string).\n",
        "        :return: commitMsg - commit message.\n",
        "        ['line', 'line', ...]\n",
        "        '''\n",
        "        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n",
        "        lines = fp.readlines()  # read all lines.\n",
        "        #numLines = len(lines)   # get the line number.\n",
        "        #print(lines)\n",
        "\n",
        "        # initialize commit message.\n",
        "        commitMsg = []\n",
        "        # get the wide range of commit message.\n",
        "        for line in lines:\n",
        "            if line.startswith('diff --git'):\n",
        "                break\n",
        "            else:\n",
        "                commitMsg.append(line)\n",
        "        #print(commitMsg)\n",
        "        # process the head of commit message.\n",
        "        while (1):\n",
        "            headMsg = commitMsg[0]\n",
        "            if (headMsg.startswith('From') or headMsg.startswith('Date:') or headMsg.startswith('Subject:')\n",
        "                    or headMsg.startswith('commit') or headMsg.startswith('Author:')):\n",
        "                commitMsg.pop(0)\n",
        "            else:\n",
        "                break\n",
        "        #print(commitMsg)\n",
        "        # process the tail of commit message.\n",
        "        dashLines = [i for i in range(len(commitMsg))\n",
        "                     if commitMsg[i].startswith('---')]  # finds all lines start with ---.\n",
        "        if (len(dashLines)):\n",
        "            lnum = dashLines[-1]  # last line number of ---\n",
        "            marks = [1 if (' file changed, ' in commitMsg[i] or ' files changed, ' in commitMsg[i]) else 0\n",
        "                     for i in range(lnum, len(commitMsg))]\n",
        "            if (sum(marks)):\n",
        "                for i in reversed(range(lnum, len(commitMsg))):\n",
        "                    commitMsg.pop(i)\n",
        "        #print(commitMsg)\n",
        "\n",
        "        #msgShow = ''\n",
        "        #for i in range(len(commitMsg)):\n",
        "        #    msgShow += commitMsg[i]\n",
        "        #print(msgShow)\n",
        "\n",
        "        return commitMsg\n",
        "\n",
        "    def ReadDiffLines(filename):\n",
        "        '''\n",
        "        Read diff code from a file.\n",
        "        :param filename:  file name (string).\n",
        "        :return: diffLines - diff code.\n",
        "        [['line', ...], ['line', ...], ...]\n",
        "        '''\n",
        "        fp = open(filename, encoding='utf-8', errors='ignore')  # get file point.\n",
        "        lines = fp.readlines()  # read all lines.\n",
        "        numLines = len(lines)  # get the line number.\n",
        "        # print(lines)\n",
        "\n",
        "        atLines = [i for i in range(numLines) if lines[i].startswith('@@ ')]  # find all lines start with @@.\n",
        "        atLines.append(numLines)\n",
        "        # print(atLines)\n",
        "\n",
        "        diffLines = []\n",
        "        for nh in range(len(atLines) - 1):  # find all hunks.\n",
        "            # print(atLines[nh], atLines[nh + 1])\n",
        "            hunk = []\n",
        "            for nl in range(atLines[nh] + 1, atLines[nh + 1]):\n",
        "                # print(lines[nl], end='')\n",
        "                if lines[nl].startswith('diff --git '):\n",
        "                    break\n",
        "                else:\n",
        "                    hunk.append(lines[nl])\n",
        "            diffLines.append(hunk)\n",
        "            # print(hunk)\n",
        "        # print(diffLines)\n",
        "        # print(len(diffLines))\n",
        "\n",
        "        # process the last hunk.\n",
        "        lastHunk = diffLines[-1]\n",
        "        numLastHunk = len(lastHunk)\n",
        "        dashLines = [i for i in range(numLastHunk) if lastHunk[i].startswith('--')]\n",
        "        if (len(dashLines)):\n",
        "            lnum = dashLines[-1]\n",
        "            for i in reversed(range(lnum, numLastHunk)):\n",
        "                lastHunk.pop(i)\n",
        "        # print(diffLines)\n",
        "        # print(len(diffLines))\n",
        "\n",
        "        return diffLines\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'filelist.txt', 'w')\n",
        "\n",
        "    # initialize data.\n",
        "    data = []\n",
        "    # read security patch data.\n",
        "    for root, ds, fs in os.walk(sDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 1])\n",
        "\n",
        "    # read positive data.\n",
        "    for root, ds, fs in os.walk(pDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 1])\n",
        "\n",
        "    # read negative data.\n",
        "    for root, ds, fs in os.walk(nDatPath):\n",
        "        for file in fs:\n",
        "            filename = os.path.join(root, file).replace('\\\\', '/')\n",
        "            fp.write(filename + '\\n')\n",
        "            commitMsg = ReadCommitMsg(filename)\n",
        "            diffLines = ReadDiffLines(filename)\n",
        "            data.append([commitMsg, diffLines, 0])\n",
        "    fp.close()\n",
        "\n",
        "    #print(len(dataLoaded))\n",
        "    #print(len(dataLoaded[0]))\n",
        "    #print(dataLoaded)\n",
        "    # [[['a', 'b', 'c', ], [['', '', '', ], ['', '', '', ], ], 0/1], ]\n",
        "    # sample = dataLoaded[i]\n",
        "    # commitMsg = dataLoaded[i][0]\n",
        "    # diffLines = dataLoaded[i][1]\n",
        "    # label = dataLoaded[i][2]\n",
        "    # diffHunk = dataLoaded[i][1][j]\n",
        "\n",
        "    # save dataLoaded.\n",
        "    if not os.path.exists(tempPath + '/data.npy'):\n",
        "        np.save(tempPath + '/data.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <ReadData> Save ' + str(len(data)) + ' raw data to ' + tempPath + '/data.npy.')\n",
        "\n",
        "    return data\n",
        "\n",
        "def GetDiffProps(data):\n",
        "    '''\n",
        "    Get the properties of the code in diff files.\n",
        "    :param data: [[[line, , ], [[line, , ], [line, , ], ...], 0/1], ...]\n",
        "    :return: props - [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def RemoveSign(line):\n",
        "        '''\n",
        "        Remove the sign (+/-) in the first character.\n",
        "        :param line: a code line.\n",
        "        :return: process line.\n",
        "        '''\n",
        "        return ' ' + line[1:] if (line[0] == '+') or (line[0] == '-') else line\n",
        "\n",
        "    def GetClangTokens(line):\n",
        "        '''\n",
        "        Get the tokens of a line with the Clang tool.\n",
        "        :param line: a code line.\n",
        "        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n",
        "                 tokenTypes - [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n",
        "                 diffTypes - [dft, dft, ...] (dft: -1, 0, 1)\n",
        "        '''\n",
        "        # remove non-ascii\n",
        "        line = line.encode(\"ascii\", \"ignore\").decode()\n",
        "\n",
        "        # defination.\n",
        "        tokenClass = [clang.cindex.TokenKind.KEYWORD,      # 1\n",
        "                      clang.cindex.TokenKind.IDENTIFIER,   # 2\n",
        "                      clang.cindex.TokenKind.LITERAL,      # 3\n",
        "                      clang.cindex.TokenKind.PUNCTUATION,  # 4\n",
        "                      clang.cindex.TokenKind.COMMENT]      # 5\n",
        "        tokenDict = {cls: index + 1 for index, cls in enumerate(tokenClass)}\n",
        "        #print(tokenDict)\n",
        "\n",
        "        # initialize.\n",
        "        tokens = []\n",
        "        tokenTypes = []\n",
        "        diffTypes = []\n",
        "\n",
        "        # clang sparser.\n",
        "        idx = clang.cindex.Index.create()\n",
        "        tu = idx.parse('tmp.cpp', args=['-std=c++11'], unsaved_files=[('tmp.cpp', RemoveSign(line))], options=0)\n",
        "        for t in tu.get_tokens(extent=tu.cursor.extent):\n",
        "            #print(t.kind, t.spelling, t.location)\n",
        "            tokens.append(t.spelling)\n",
        "            tokenTypes.append(tokenDict[t.kind])\n",
        "            diffTypes.append(1 if (line[0] == '+') else -1 if (line[0] == '-') else 0)\n",
        "        #print(tokens)\n",
        "        #print(tokenTypes)\n",
        "        #print(diffTypes)\n",
        "\n",
        "        return tokens, tokenTypes, diffTypes\n",
        "\n",
        "    def GetWordTokens(line):\n",
        "        '''\n",
        "        Get the word tokens from a code line.\n",
        "        :param line: a code line.\n",
        "        :return: tokens - ['tk', 'tk', ...] ('tk': string)\n",
        "        '''\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokens = tknzr.tokenize(RemoveSign(line))\n",
        "        return tokens\n",
        "\n",
        "    def GetString(lines):\n",
        "        '''\n",
        "        Get the strings from the diff code\n",
        "        :param lines: diff code.\n",
        "        :return: lineStr - All the diff lines.\n",
        "                 lineStrB - The before-version code lines.\n",
        "                 lineStrA - The after-version code lines.\n",
        "        '''\n",
        "        lineStr = ''\n",
        "        lineStrB = ''\n",
        "        lineStrA = ''\n",
        "        for hunk in lines:\n",
        "            for line in hunk:\n",
        "                # all lines.\n",
        "                lineStr += RemoveSign(line)\n",
        "                # all Before lines.\n",
        "                lineStrB += RemoveSign(line) if line[0] != '+' else ''\n",
        "                # all After lines.\n",
        "                lineStrA += RemoveSign(line) if line[0] != '-' else ''\n",
        "\n",
        "        return lineStr, lineStrB, lineStrA\n",
        "\n",
        "    def GetDiffTokens(lines):\n",
        "        '''\n",
        "        Get the tokens for the diff lines.\n",
        "        :param lines: the diff code.\n",
        "        :return: tokens - tokens ['tk', 'tk', ...] ('tk': string)\n",
        "                 tokenTypes - token types [tkt, tkt, ...] (tkt: 1, 2, 3, 4, 5)\n",
        "                 diffTypes - diff types [dft, dft, ...] (dft: -1, 0, 1)\n",
        "        '''\n",
        "        # initialize.\n",
        "        tokens = []\n",
        "        tokenTypes = []\n",
        "        diffTypes = []\n",
        "\n",
        "        # for each line of lines.\n",
        "        for hunk in lines:\n",
        "            for line in hunk:\n",
        "                #print(line, end='')\n",
        "                tk, tkT, dfT = GetClangTokens(line)\n",
        "                tokens.extend(tk)\n",
        "                tokenTypes.extend(tkT)\n",
        "                diffTypes.extend(dfT)\n",
        "                #print('-----------------------------------------------------------------------')\n",
        "        #print(tokens)\n",
        "        #print(tokenTypes)\n",
        "        #print(diffTypes)\n",
        "\n",
        "        return tokens, tokenTypes, diffTypes\n",
        "\n",
        "    #lines = data[0][1]\n",
        "    #print(lines)\n",
        "    #hunk = data[0][1][0]\n",
        "    #print(hunk)\n",
        "    #line = data[0][1][0][0]\n",
        "    #print(line)\n",
        "\n",
        "    # for each sample data[n].\n",
        "    numData = len(data)\n",
        "    props = []\n",
        "    for n in range(numData):\n",
        "        # get the lines of the diff file.\n",
        "        diffLines = data[n][1]\n",
        "        # properties.\n",
        "        tk, tkT, dfT = GetDiffTokens(diffLines)\n",
        "        label = data[n][2]\n",
        "        prop = [tk, tkT, dfT, label]\n",
        "        #print(prop)\n",
        "        props.append(prop)\n",
        "        print(n)\n",
        "\n",
        "    # save dataLoaded.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    if not os.path.exists(tempPath + '/props.npy'):\n",
        "        np.save(tempPath + '/props.npy', props, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffProps> Save ' + str(len(props)) + ' diff property data to ' + tempPath + '/props.npy.')\n",
        "\n",
        "    return props\n",
        "\n",
        "def GetDiffVocab(props):\n",
        "    '''\n",
        "    Get the vocabulary of diff tokens\n",
        "    :param props - the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :return: vocab - the vocabulary of diff tokens. ['tk', 'tk', ...]\n",
        "             maxLen - the max length of a diff code.\n",
        "    '''\n",
        "\n",
        "    # create temp folder.\n",
        "    if not os.path.exists(tempPath):\n",
        "        os.mkdir(tempPath)\n",
        "    fp = open(tempPath + 'difflen.csv', 'w')\n",
        "\n",
        "    # get the whole tokens and the max diff length.\n",
        "    tokens = []\n",
        "    maxLen = 0\n",
        "\n",
        "    # for each sample.\n",
        "    for item in props:\n",
        "        tokens.extend(item[0])\n",
        "        maxLen = len(item[0]) if (len(item[0]) > maxLen) else maxLen\n",
        "        fp.write(str(len(item[0])) + '\\n')\n",
        "    fp.close()\n",
        "\n",
        "    # remove duplicates and get vocabulary.\n",
        "    vocab = {}.fromkeys(tokens)\n",
        "    vocab = list(vocab.keys())\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffVocab> There are ' + str(len(vocab)) + ' diff vocabulary tokens. (except \\'<pad>\\')')\n",
        "    print('[INFO] <GetDiffVocab> The max diff length is ' + str(maxLen) + ' tokens. (hyperparameter: _DiffMaxLen_ = ' + str(_DiffMaxLen_) + ')')\n",
        "\n",
        "    return vocab, maxLen\n",
        "\n",
        "def GetDiffDict(vocab):\n",
        "    '''\n",
        "    Get the dictionary of diff vocabulary.\n",
        "    :param vocab: the vocabulary of diff tokens. ['tk', 'tk', ...]\n",
        "    :return: tokenDict - the dictionary of diff vocabulary.\n",
        "    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n",
        "    '''\n",
        "\n",
        "    # get token dict from vocabulary.\n",
        "    tokenDict = {token: index for index, token in enumerate(vocab)}\n",
        "    tokenDict['<pad>'] = len(tokenDict)\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffDict> Create dictionary for ' + str(len(tokenDict)) + ' diff vocabulary tokens. (with \\'<pad>\\')')\n",
        "\n",
        "    return tokenDict\n",
        "\n",
        "def GetDiffEmbed(tokenDict, embedSize):\n",
        "    '''\n",
        "    Get the pre-trained weights for embedding layer from the dictionary of diff vocabulary.\n",
        "    :param tokenDict: the dictionary of diff vocabulary.\n",
        "    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n",
        "    :param embedSize: the dimension of the embedding vector.\n",
        "    :return: preWeights - the pre-trained weights for embedding layer.\n",
        "    [[n, ...], [n, ...], ...]\n",
        "    '''\n",
        "\n",
        "    # number of the vocabulary tokens.\n",
        "    numTokens = len(tokenDict)\n",
        "    # initialize the pre-trained weights for embedding layer.\n",
        "    preWeights = np.zeros((numTokens, embedSize))\n",
        "    for index in range(numTokens):\n",
        "        preWeights[index] = np.random.normal(size=(embedSize,))\n",
        "    print('[INFO] <GetDiffEmbed> Create pre-trained embedding weights with ' + str(len(preWeights)) + ' * ' + str(len(preWeights[0])) + ' matrix.')\n",
        "\n",
        "    # save preWeights.\n",
        "    if not os.path.exists(tempPath + '/preWeights.npy'):\n",
        "        np.save(tempPath + '/preWeights.npy', preWeights, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffEmbed> Save the pre-trained weights of embedding layer to ' + tempPath + '/preWeights.npy.')\n",
        "\n",
        "    return preWeights\n",
        "\n",
        "def GetDiffMapping(props, maxLen, tokenDict):\n",
        "    '''\n",
        "    Map the feature data into indexed data.\n",
        "    :param props: the features of diff code.\n",
        "    [[[tokens], [nums], [nums], 0/1], ...]\n",
        "    :param maxLen: the max length of a diff code.\n",
        "    :param tokenDict: the dictionary of diff vocabulary.\n",
        "    {'tk': 0, 'tk': 1, ..., '<pad>': N}\n",
        "    :return: np.array(data) - feature data.\n",
        "             [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "             np.array(labels) - labels.\n",
        "             [[0/1], ...]\n",
        "    '''\n",
        "\n",
        "    def PadList(dList, pad, length):\n",
        "        '''\n",
        "        Pad the list data to a fixed length.\n",
        "        :param dList: the list data - [ , , ...]\n",
        "        :param pad: the variable used to pad.\n",
        "        :param length: the fixed length.\n",
        "        :return: dList - padded list data. [ , , ...]\n",
        "        '''\n",
        "\n",
        "        if len(dList) <= length:\n",
        "            dList.extend(pad for i in range(length - len(dList)))\n",
        "        elif len(dList) > length:\n",
        "            dList = dList[0:length]\n",
        "        return dList\n",
        "\n",
        "    # initialize the data and labels.\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    # for each sample.\n",
        "    for item in props:\n",
        "        # initialize sample.\n",
        "        sample = []\n",
        "\n",
        "        # process token.\n",
        "        tokens = item[0]\n",
        "        tokens = PadList(tokens, '<pad>', maxLen)\n",
        "        tokens2index = []\n",
        "        for tk in tokens:\n",
        "            tokens2index.append(tokenDict[tk])\n",
        "        sample.append(tokens2index)\n",
        "        # process tokenTypes.\n",
        "        tokenTypes = item[1]\n",
        "        tokenTypes = PadList(tokenTypes, 0, maxLen)\n",
        "        sample.append(tokenTypes)\n",
        "        # process diffTypes.\n",
        "        diffTypes = item[2]\n",
        "        diffTypes = PadList(diffTypes, 0, maxLen)\n",
        "        sample.append(diffTypes)\n",
        "\n",
        "        # process sample.\n",
        "        sample = np.array(sample).T\n",
        "        data.append(sample)\n",
        "        # process label.\n",
        "        label = item[3]\n",
        "        labels.append([label])\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] data:')\n",
        "        print(data[0:3])\n",
        "        print('[DEBUG] labels:')\n",
        "        print(labels[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <GetDiffMapping> Create ' + str(len(data)) + ' feature data with ' + str(len(data[0])) + ' * ' + str(len(data[0][0])) + ' matrix.')\n",
        "    print('[INFO] <GetDiffMapping> Create ' + str(len(labels)) + ' labels with 1 * 1 matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/ndata_' + str(maxLen) + '.npy')) \\\n",
        "            | (not os.path.exists(tempPath + '/nlabels_' + str(maxLen) + '.npy')):\n",
        "        np.save(tempPath + '/ndata_' + str(maxLen) + '.npy', data, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffMapping> Save the mapped numpy data to ' + tempPath + '/ndata_' + str(maxLen) + '.npy.')\n",
        "        np.save(tempPath + '/nlabels_' + str(maxLen) + '.npy', labels, allow_pickle=True)\n",
        "        print('[INFO] <GetDiffMapping> Save the mapped numpy labels to ' + tempPath + '/nlabels_' + str(maxLen) + '.npy.')\n",
        "\n",
        "    return np.array(data), np.array(labels)\n",
        "\n",
        "def UpdateTokenTypes(data):\n",
        "    '''\n",
        "    Update the token type in the feature data into one-hot vector.\n",
        "    :param data: feature data. [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "    :return: np.array(newData). [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    '''\n",
        "\n",
        "    newData = []\n",
        "    # for each sample.\n",
        "    for item in data:\n",
        "        # get the transpose of props.\n",
        "        itemT = item.T\n",
        "        # initialize new sample.\n",
        "        newItem = []\n",
        "        newItem.append(itemT[0])\n",
        "        newItem.extend(np.zeros((5, len(item)), dtype=int))\n",
        "        newItem.append(itemT[2])\n",
        "        # assign the new sample.\n",
        "        for i in range(len(item)):\n",
        "            tokenType = itemT[1][i]\n",
        "            if (tokenType):\n",
        "                newItem[tokenType][i] = 1\n",
        "        # get the transpose of new sample.\n",
        "        newItem = np.array(newItem).T\n",
        "        # append new sample.\n",
        "        newData.append(newItem)\n",
        "\n",
        "    if _DEBUG_:\n",
        "        print('[DEBUG] newData:')\n",
        "        print(newData[0:3])\n",
        "\n",
        "    # print.\n",
        "    print('[INFO] <UpdateTokenTypes> Update ' + str(len(newData)) + ' feature data with ' + str(len(newData[0])) + ' * ' + str(len(newData[0][0])) + ' matrix.')\n",
        "\n",
        "    # save files.\n",
        "    if (not os.path.exists(tempPath + '/newdata_' + str(len(newData[0])) + '.npy')):\n",
        "        np.save(tempPath + '/newdata_' + str(len(newData[0])) + '.npy', newData, allow_pickle=True)\n",
        "        print('[INFO] <UpdateTokenTypes> Save the mapped numpy data to ' + tempPath + '/newdata_' + str(len(newData[0])) + '.npy.')\n",
        "\n",
        "    # change marco.\n",
        "    global _DiffExtraDim_\n",
        "    _DiffExtraDim_ = 6\n",
        "\n",
        "    return np.array(newData)\n",
        "\n",
        "def SplitData(data, labels, setType, rate=0.2):\n",
        "    '''\n",
        "    Split the data and labels into two sets with a specific rate.\n",
        "    :param data: feature data.\n",
        "    [[[n, {0~5}, {-1~1}], ...], ...]\n",
        "    [[[n, 0/1, 0/1, 0/1, 0/1, 0/1, {-1~1}], ...], ...]\n",
        "    :param labels: labels. [[0/1], ...]\n",
        "    :param setType: the splited dataset type.\n",
        "    :param rate: the split rate. 0 ~ 1\n",
        "    :return: dsetRest - the rest dataset.\n",
        "             lsetRest - the rest labels.\n",
        "             dset - the splited dataset.\n",
        "             lset - the splited labels.\n",
        "    '''\n",
        "\n",
        "    # set parameters.\n",
        "    setType = setType.upper()\n",
        "    numData = len(data)\n",
        "    num = math.floor(numData * rate)\n",
        "\n",
        "    # get the random data list.\n",
        "    if (os.path.exists(tempPath + '/split_' + setType + '.npy')) & (_LOCK_):\n",
        "        dataList = np.load(tempPath + '/split_' + setType + '.npy')\n",
        "    else:\n",
        "        dataList = list(range(numData))\n",
        "        random.shuffle(dataList)\n",
        "        np.save(tempPath + '/split_' + setType + '.npy', dataList, allow_pickle=True)\n",
        "\n",
        "    # split data.\n",
        "    dset = data[dataList[0:num]]\n",
        "    lset = labels[dataList[0:num]]\n",
        "    dsetRest = data[dataList[num:]]\n",
        "    lsetRest = labels[dataList[num:]]\n",
        "\n",
        "    # print.\n",
        "    setTypeRest = 'TRAIN' if (setType == 'VALID') else 'REST'\n",
        "    print('[INFO] <SplitData> Split data into ' + str(len(dsetRest)) + ' ' + setTypeRest\n",
        "          + ' dataset and ' + str(len(dset)) + ' ' + setType + ' dataset. (Total: '\n",
        "          + str(len(dsetRest) + len(dset)) + ', Rate: ' + str(int(rate * 100)) + '%)')\n",
        "\n",
        "    return dsetRest, lsetRest, dset, lset\n",
        "\n",
        "class TextRNN(nn.Module):\n",
        "    def __init__(self, preWeights, hiddenSize=32, hiddenLayers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "        # parameters.\n",
        "        class_num = 2\n",
        "        vocabSize, embedDim = preWeights.size()\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(num_embeddings=vocabSize, embedding_dim=embedDim)\n",
        "        self.embedding.load_state_dict({'weight': preWeights})\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        # LSTM Layer\n",
        "        _DiffExtraDim_ = 6\n",
        "        self.lstm = nn.LSTM(input_size=embedDim+_DiffExtraDim_, hidden_size=hiddenSize, num_layers=hiddenLayers, bidirectional=True)\n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(hiddenSize * hiddenLayers * 2, class_num)\n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x             batch_size * diff_length * feature_dim\n",
        "        embeds = self.embedding(x[:,:,0])\n",
        "        # embeds        batch_size * diff_length * embedding_dim\n",
        "        features = x[:, :, 1:]\n",
        "        # features      batch_size * diff_length * _DiffExtraDim_\n",
        "        inputs = torch.cat((embeds.float(), features.float()), 2)\n",
        "        # inputs        batch_size * diff_length * (embedding_dim + _DiffExtraDim_)\n",
        "        inputs = inputs.permute(1, 0, 2)\n",
        "        # inputs        diff_length * batch_size * (embedding_dim + _DiffExtraDim_)\n",
        "        lstm_out, (h_n, c_n) = self.lstm(inputs)\n",
        "        # lstm_out      diff_length * batch_size * (hidden_size * direction_num)\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        # h_n           (num_layers * direction_num) * batch_size * hidden_size\n",
        "        feature_map = torch.cat([h_n[i, :, :] for i in range(h_n.shape[0])], dim=1)\n",
        "        # feature_map   batch_size * (hidden_size * num_layers * direction_num)\n",
        "        final_out = self.fc(feature_map)    # batch_size * class_num\n",
        "        return self.softmax(final_out)      # batch_size * class_num\n",
        "\n",
        "def TextRNNTrain(dTrain, lTrain, dValid, lValid, preWeights, batchsize=64, learnRate=0.001, dTest=None, lTest=None):\n",
        "\n",
        "    # get the mark of the test dataset.\n",
        "    if dTest is None: dTest = []\n",
        "    if lTest is None: lTest = []\n",
        "    markTest = 1 if (len(dTest)) & (len(lTest)) else 0\n",
        "\n",
        "    # tensor data processing.\n",
        "    xTrain = torch.from_numpy(dTrain).long().cuda()\n",
        "    yTrain = torch.from_numpy(lTrain).long().cuda()\n",
        "    xValid = torch.from_numpy(dValid).long().cuda()\n",
        "    yValid = torch.from_numpy(lValid).long().cuda()\n",
        "    if (markTest):\n",
        "        xTest = torch.from_numpy(dTest).long().cuda()\n",
        "        yTest = torch.from_numpy(lTest).long().cuda()\n",
        "    # batch size processing.\n",
        "    train = torchdata.TensorDataset(xTrain, yTrain)\n",
        "    trainloader = torchdata.DataLoader(train, batch_size=batchsize, shuffle=False)\n",
        "    valid = torchdata.TensorDataset(xValid, yValid)\n",
        "    validloader = torchdata.DataLoader(valid, batch_size=batchsize, shuffle=False)\n",
        "    if (markTest):\n",
        "        test = torchdata.TensorDataset(xTest, yTest)\n",
        "        testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "\n",
        "    # build the model of recurrent neural network.\n",
        "    preWeights = torch.from_numpy(preWeights)\n",
        "    model = TextRNN(preWeights, hiddenSize=_TRnnHidSiz_, hiddenLayers=_TRnnHidLay_)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    print('[INFO] <TextRNNTrain> ModelType: TextRNN, HiddenNodes: %d, HiddenLayers: %d.' % (_TRnnHidSiz_, _TRnnHidLay_))\n",
        "    print('[INFO] <TextRNNTrain> BatchSize: %d, LearningRate: %.4f, MaxEpoch: %d, PerEpoch: %d.' % (batchsize, learnRate, _TRnnMaxEpoch_, _TRnnPerEpoch_))\n",
        "    # optimizing with stochastic gradient descent.\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learnRate)\n",
        "    # seting loss function as mean squared error.\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    # memory\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    torch.backends.cudnn.enabled = True\n",
        "\n",
        "    # run on each epoch.\n",
        "    accList = [0]\n",
        "    for epoch in range(_TRnnMaxEpoch_):\n",
        "        # training phase.\n",
        "        model.train()\n",
        "        lossTrain = 0\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        for iter, (data, label) in enumerate(trainloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()  # set the gradients to zero.\n",
        "            yhat = model.forward(data)  # get output\n",
        "            loss = criterion(yhat, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # statistic\n",
        "            lossTrain += loss.item() * len(label)\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        lossTrain /= len(dTrain)\n",
        "        # train accuracy.\n",
        "        accTrain = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # validation phase.\n",
        "        model.eval()\n",
        "        predictions = []\n",
        "        labels = []\n",
        "        with torch.no_grad():\n",
        "            for iter, (data, label) in enumerate(validloader):\n",
        "                data = data.to(device)\n",
        "                label = label.contiguous().view(-1)\n",
        "                label = label.to(device)\n",
        "                yhat = model.forward(data)  # get output\n",
        "                # statistic\n",
        "                preds = yhat.max(1)[1]\n",
        "                predictions.extend(preds.int().tolist())\n",
        "                labels.extend(label.int().tolist())\n",
        "                torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        # valid accuracy.\n",
        "        accValid = accuracy_score(labels, predictions) * 100\n",
        "        accList.append(accValid)\n",
        "\n",
        "        if (markTest):\n",
        "            # testing phase.\n",
        "            model.eval()\n",
        "            predictions = []\n",
        "            labels = []\n",
        "            with torch.no_grad():\n",
        "                for iter, (data, label) in enumerate(testloader):\n",
        "                    data = data.to(device)\n",
        "                    label = label.contiguous().view(-1)\n",
        "                    label = label.to(device)\n",
        "                    yhat = model.forward(data)  # get output\n",
        "                    # statistic\n",
        "                    preds = yhat.max(1)[1]\n",
        "                    predictions.extend(preds.int().tolist())\n",
        "                    labels.extend(label.int().tolist())\n",
        "                    torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "            # test accuracy.\n",
        "            accTest = accuracy_score(labels, predictions) * 100\n",
        "\n",
        "        # output information.\n",
        "        if (0 == (epoch + 1) % _TRnnPerEpoch_):\n",
        "            strAcc = '[Epoch {:03X}] loss: {:.3}, train acc: {:.3f}%, valid acc: {:.3f}%.'.format(epoch + 1, lossTrain, accTrain, accValid)\n",
        "            if (markTest):\n",
        "                strAcc = strAcc[:-1] + ', test acc: {:.3f}%.'.format(accTest)\n",
        "            print(strAcc)\n",
        "        # save the best model.\n",
        "        if (accList[-1] > max(accList[0:-1])):\n",
        "            torch.save(model.state_dict(), tempPath + '/model_TextRNN.pth')\n",
        "        # stop judgement.\n",
        "        if (epoch >= _TRnnJudEpoch_) and (accList[-1] < min(accList[-1-_TRnnJudEpoch_:-1])):\n",
        "            break\n",
        "\n",
        "    # load best model.\n",
        "    model.load_state_dict(torch.load(tempPath + '/model_TextRNN.pth'))\n",
        "    print('[INFO] <TextRNNTrain> Finish training TextRNN model. (Best model: ' + tempPath + '/model_TextRNN.pth)')\n",
        "\n",
        "    return model\n",
        "\n",
        "def TextRNNTest(model, dTest, lTest, batchsize=64):\n",
        "    # tensor data processing.\n",
        "    xTest = torch.from_numpy(dTest).long().cuda()\n",
        "    yTest = torch.from_numpy(lTest).long().cuda()\n",
        "    # batch size processing.\n",
        "    test = torchdata.TensorDataset(xTest, yTest)\n",
        "    testloader = torchdata.DataLoader(test, batch_size=batchsize, shuffle=False)\n",
        "    # load the model of recurrent neural network.\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # testing phase.\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for iter, (data, label) in enumerate(testloader):\n",
        "            data = data.to(device)\n",
        "            label = label.contiguous().view(-1)\n",
        "            label = label.to(device)\n",
        "            yhat = model.forward(data)  # get output\n",
        "            # statistic\n",
        "            preds = yhat.max(1)[1]\n",
        "            predictions.extend(preds.int().tolist())\n",
        "            labels.extend(label.int().tolist())\n",
        "            torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # testing accuracy.\n",
        "    accuracy = accuracy_score(labels, predictions) * 100\n",
        "    predictions = [[item] for item in predictions]\n",
        "\n",
        "    return predictions, accuracy\n",
        "\n",
        "def OutputEval(predictions, labels, method=''):\n",
        "\n",
        "    # evaluate the predictions with gold labels, and get accuracy and confusion matrix.\n",
        "    def Evaluation(predictions, labels):\n",
        "\n",
        "        # parameter settings.\n",
        "        D = len(labels)\n",
        "        cls = 2\n",
        "\n",
        "        # get confusion matrix.\n",
        "        confusion = np.zeros((cls, cls))\n",
        "        for ind in range(D):\n",
        "            nRow = int(predictions[ind][0])\n",
        "            nCol = int(labels[ind][0])\n",
        "            confusion[nRow][nCol] += 1\n",
        "\n",
        "        # get accuracy.\n",
        "        accuracy = 0\n",
        "        for ind in range(cls):\n",
        "            accuracy += confusion[ind][ind]\n",
        "        accuracy /= D\n",
        "\n",
        "        return accuracy, confusion\n",
        "\n",
        "    # get accuracy and confusion matrix.\n",
        "    accuracy, confusion = Evaluation(predictions, labels)\n",
        "\n",
        "    # output on screen and to file.\n",
        "    print('       -------------------------------------------')\n",
        "    if len(method): print('       method   : ' +  method)\n",
        "    print('       accuracy : %.3f%%' % (accuracy * 100))\n",
        "    print('       confusion matrix :      (actual)')\n",
        "    print('                           Neg         Pos')\n",
        "    print('       (predicted) Neg     %-5d(TN)   %-5d(FN)' % (confusion[0][0], confusion[0][1]))\n",
        "    print('                   Pos     %-5d(FP)   %-5d(TP)' % (confusion[1][0], confusion[1][1]))\n",
        "    print('       -------------------------------------------')\n",
        "\n",
        "    return accuracy, confusion\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    demoTextRNN()\n",
        "    #diffData = np.load(tempPath + '/newdata_' + str(_DiffMaxLen_) + '.npy')\n",
        "    #diffLabels = np.load(tempPath + '/nlabels_' + str(_DiffMaxLen_) + '.npy')\n",
        "    #dataRest, labelRest, dataTest, labelTest = SplitData(diffData, diffLabels, 'test', rate=0.2)\n",
        "    #dataTrain, labelTrain, dataValid, labelValid = SplitData(dataRest, labelRest, 'valid', rate=0.2)\n",
        "    #diffPreWeights = np.load(tempPath + '/preWeights.npy')\n",
        "    #if (_MODEL_) & (os.path.exists(tempPath + '/model_TextRNN.pth')):\n",
        "    #    preWeights = torch.from_numpy(diffPreWeights)\n",
        "    #    model = TextRNN(preWeights, hiddenSize=_TRnnHidSiz_, hiddenLayers=_TRnnHidLay_)\n",
        "    #    model.load_state_dict(torch.load(tempPath + '/model_TextRNN.pth'))\n",
        "    #else:\n",
        "    #    model = TextRNNTrain(dataTrain, labelTrain, dataValid, labelValid, preWeights=diffPreWeights,\n",
        "    #                         batchsize=_TRnnBatchSz_, learnRate=_TRnnLearnRt_, dTest=dataTest, lTest=labelTest)\n",
        "    #predictions, accuracy = TextRNNTest(model, dataTest, labelTest, batchsize=_TRnnBatchSz_)\n",
        "    #_, confusion = OutputEval(predictions, labelTest, 'TextRNN')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}